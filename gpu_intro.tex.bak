%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% GPU
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \cleardoublepage
\chapter{Signal Processing with GPUs}
\label{sec:gpu}

This thesis explores the use of GPUs in data-aided estimation, equalization and filtering operations.

A Graphics Processing Unit (GPU) is a computational unit with a highly-parallel architecture well-suited for executing the same function on many data elements.
In the past, GPUs were used to process graphics data.
Recently, general purpose GPUs are being used for high performance computing in computer vision, deep learning, artificial intelligence and signal processing \cite{wikipedia-gpu:2015}.

GPUs cannot be programmed the way as a CPU. 
NVIDIA released a extension to C, C++ and Fortran called CUDA (Compute Unified Device Architecture).
CUDA allows a programmer to write C++ like functions that are massively parallel called \textit{kernels}.
To invoke parallelism, a GPU kernel is called $N$ times and mapped to $N$ \textit{threads} that run concurrently.
To achieve the full potential of high performance GPUs, kernels must be written with some basic concepts about GPU architecture and memory in mind.

The purpose of this overview is to provide context for the contributions of this thesis.
As such this overview is not a tutorial.
For a full explination of CUDA programming please see the CUDA toolkit documentation \cite{CUDA_toolkit_doc}.

\section{Simple GPU code example}
If a programmer has some C++ experience, learning how to program GPUs using CUDA comes fairly easily.
GPU code still runs top to bottom and memory still has to be allocated.
The only real difference is where the memory physically is and how functions run on GPUs.
To run functions or kernels on GPUs, the memory must be copied from the host (CPU) to the device (GPU).
Once the memory has been copied, the parallel GPU kernels can be called.
After the GPU kernels have finished, the resulting memory has to be copied back from the device (GPU) to the host (CPU).

Listing \ref{code:GPUvsCPU} shows a simple program that sums to vectors together
\begin{equation}
\begin{matrix}
\mathbf{C}_1 = \mathbf{A}_1 + \mathbf{B}_1 \\
\mathbf{C}_2 = \mathbf{A}_2 + \mathbf{B}_2
\end{matrix}
\end{equation}
where each vector is length $1024$.
Figure \ref{fig:CPUaddBlockDiagram} shows how the CPU computes $\mathbf{C}_1$ by summing elements of $\mathbf{A}_1$ and $\mathbf{B}_1$ together \textit{sequentially}.
Figure \ref{fig:GPUaddBlockDiagram} shows how the GPU computes $\mathbf{C}_2$ by summing elements of $\mathbf{A}_1$ and $\mathbf{B}_1$ together \textit{in parallel}.
The GPU kernel computes every element of $\mathbf{C}_2$ in parallel while the CPU computes one element of $\mathbf{C}_1$ at a time.
\begin{figure}
	\centering\includegraphics[width=2.38in/100*55]{figures/gpu_intro/CPUaddBlockDiagram.pdf}
	\caption{A block diagram of how a CPU sequentially performs vector addition.}
	\label{fig:CPUaddBlockDiagram}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=3.91in/100*55]{figures/gpu_intro/GPUaddBlockDiagram.pdf}
	\caption{A block diagram of how a GPU performs vector addition in parallel.}
	\label{fig:GPUaddBlockDiagram}
\end{figure}
\singlespacing
\clearpage
\begin{lstlisting}[caption={Comparison of CPU verse GPU code.},label={code:GPUvsCPU}]
#include <iostream>
#include <stdlib.h>
using namespace std;

void VecAddCPU(int* destination,int* source0,int* source1,int myLength){
	for(int i = 0; i < myLength; i++)
		destination[i] = source0[i] + source1[i];
}

__global__ void VecAddGPU(int* destination, int* source0, int* source1, int lastThread){
	int i = blockIdx.x*blockDim.x + threadIdx.x;
	
	// don't access elements out of bounds
	if(i >= lastThread)
		return;
	
	destination[i] = source0[i] + source1[i];
}

int main(){
	int numPoints = 1024;
	/*--------------------------------------------------------------------
                               	   CPU Start
	--------------------------------------------------------------------*/
	// allocate memory on host
	int *A1;
	int *B1;
	int *C1;
	A1 = (int*) malloc (numPoints*sizeof(int));
	B1 = (int*) malloc (numPoints*sizeof(int));
	C1 = (int*) malloc (numPoints*sizeof(int));

	// Initialize vectors 0-99
	for(int i = 0; i < numPoints; i++){
		A1[i] = rand()%100;
		B1[i] = rand()%100;
	}

	// vector sum C1 = A1 + B1
	VecAddCPU(C1, A1, B1, numPoints);
	/*--------------------------------------------------------------------
                               	   CPU End
	--------------------------------------------------------------------*/

	/*--------------------------------------------------------------------
                               	   GPU End
	--------------------------------------------------------------------*/
	// allocate memory on host for result
	int *C2;
	C2 = (int*) malloc (numPoints*sizeof(int));

	// allocate memory on device for computation
	int *A2_gpu;
	int *B2_gpu;
	int *C2_gpu;
	cudaMalloc(&A2_gpu, sizeof(int)*numPoints);
	cudaMalloc(&B2_gpu, sizeof(int)*numPoints);
	cudaMalloc(&C2_gpu, sizeof(int)*numPoints);

	// Copy vectors A and B from host to device
	cudaMemcpy(A2_gpu, A1, sizeof(int)*numPoints, cudaMemcpyHostToDevice);
	cudaMemcpy(B2_gpu, B1, sizeof(int)*numPoints, cudaMemcpyHostToDevice);

	// Set optimal number of threads per block
	int numTreadsPerBlock = 32;

	// Compute number of blocks for set number of threads
	int numBlocks = numPoints/numTreadsPerBlock;

	// If there are left over points, run an extra block
	if(numPoints % numTreadsPerBlock > 0)
		numBlocks++;

	// Run computation on device
	VecAddGPU<<<numBlocks, numTreadsPerBlock>>>(C2_gpu, A2_gpu, B2_gpu, numPoints);

	// Copy vector C2 from device to host
	cudaMemcpy(C2, C2_gpu, sizeof(int)*numPoints, cudaMemcpyDeviceToHost);
	/*--------------------------------------------------------------------
                               	   GPU End
	--------------------------------------------------------------------*/

	// Free vectors on CPU
	free(A1);
	free(B1);
	free(C1);
	free(C2);

	// Free vectors on GPU
	cudaFree(A2_gpu);
	cudaFree(B2_gpu);
	cudaFree(C2_gpu);
}
\end{lstlisting}
\doublespacing

\section{GPU kernel using threads and thread blocks}
A GPU kernel is executed on a GPU by launching numTreadsPerBlock$\times$numBlocks threads.
Each thread has a unique index.
CUDA calls this indes threadIdx and blockIdx.
threadIdx is the thread index inside the assigned thread block.
blockIdx is the index of the block the thread is assigned.
blockDim is the number of threads assigned per block, in fact blockDim $=$ numTreadsPerBlock.
Both threadIdx and blockIdx are three dimensional and have x, y and z components.
In this thesis only the x dimension is used because GPU kernels operate only on vectors.

To replace a CPU for loop that runs $0$ to $N-1$, a GPU kernel launches $N$ threads are with $T$ threads per thread block.
The number of blocks need is $M = \frac{N}{T}$ or $M = \frac{N}{T}+1$ if $N$ is not an integer multiple of $T$.
Figure \ref{fig:threadsBlocks32} shows $32$ threads launched in $4$ thread blocks with $8$ threads per block.
Figure \ref{fig:threadsBlocks36} shows $36$ threads launched in $5$ thread blocks with $8$ threads per block. An full extra thread block must be launched to with $8$ threads but $4$ threads are idle.
\begin{figure}
	\centering\includegraphics[width=4in/100*55]{figures/gpu_intro/threadsBlocks32.pdf}
	\caption{Block $0$ $32$ threads launched in $4$ thread blocks with $8$ threads per block.}
	\label{fig:threadsBlocks32}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=5in/100*55]{figures/gpu_intro/threadsBlocks36.pdf}
	\caption{$36$ threads launched in $5$ thread blocks with $8$ threads per block with $4$ idle threads.}
	\label{fig:threadsBlocks36}
\end{figure}

\section{GPU memory}
Thread blocks run independent of other thread blocks.
The GPU does not guarantee Block $0$ will execute before Block $2$.
Threads in blocks can coordinate and use shared memory but blocks do not coordinate with other blocks.
Threads have access to private local memory that is fast and efficient.
Each thread in a thread block has access to private shared memory in the thread block.
All threads have access to global memory.

Local memory is the fastest and global memory is by far the slowest.
One global memory access takes 400-800 clock cycles while a local memory is a few clock cycles.
Why not just do all computations in local memory?
The memory needs come from global memory to before it can be used in local memory.
Memory should be saved in shared memory if many threads are going to use it in a thread block.
Local and shared memory should be used as much as possible but sometimes a GPU kernel cant utilized local and shared memory because elements might only be used once.
\begin{sidewaysfigure}
	\centering\includegraphics[width=9in/100*55]{figures/gpu_intro/fullGPUmemBlockDiagram.png}
	\caption{A block diagram where local, shared, and global memory is located. Each thread has private local memory. Each thread block has private shared memory. The GPU has global memory that all threads can access.}
	\label{fig:fullGPUmemBlockDiagram}
\end{sidewaysfigure}

Why is global memory so slow?
Looking at the physical hardware will shed some light.
This thesis uses NVIDIA Tesla K40c and K20c GPUs, Figure \ref{fig:GPUpicture} shows the form factor of the these GPUs.
The red box in Figure \ref{fig:GPUarch} show the GPU chip and the yellow boxes show the SRAM that is \textit{off} the GPU chip.
The GPU global memory is located in the SRAM.
To move memory to thread blocks \textit{on} the GPU chip from global memory requires fetching memory from \textit{off} the GPU.
Now 400-800 clock cycles doesn't sound all the bad huh?
\begin{figure}
	\centering\includegraphics[width=5in]{figures/gpu_intro/k40c_k20c.jpg}
	\caption{NVIDIA Tesla K40c and K20c.}
	\label{fig:GPUpicture}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=\textwidth]{figures/gpu_intro/Kepler_box.png}
	\caption{Example of an NVIDIA GPU card. The SRAM is shown to be boxed in yellow. The GPU chip is shown to be boxed in red.}
	\label{fig:GPUarch}
\end{figure}

Improving memory accesses should always be the first optimization when a GPU kernel needs to be faster.
The next step is to find the optimal number of threads per block to launch.
The number of threads per block affect the amount of resources available to each thread.
If a kernel is more computationally heavy, launching less threads per block may lead to faster execution time because threads have more resources available. 
A kernel can also be extremely computationally light, like VecAddGPU, making each thread need very little resources.

Luckily, there is a finite number of possible threads per block, $1$ to $1024$.
A simple test program could time a GPU kernel while sweeping the number of threads per block from $1$ to $1024$.
The test program will produce the optimal number of threads per block for that specific GPU kernel.


a simple program similarities can be seen between CPU and GPU coding.
The main difference between CPU and GPU coding is how computations are done.
The CPU function on line 40 (defined on lines 5-8) is called once and it runs once.
The GPU kernel on line 76 (defined on lines 10-18) is called once but it runs numBlocks$\times$numTreadsPerBlock times in parallel.

Each thread that executes the kernel is given a threadIdx and a blockDim with a blockIdx.
Each of these integers are three dimensional, but only one dimension is used in this thesis.
The integer threadIdx is the index of the thread inside the block.
The integer blockDim is the number of threads per block launched.
The integer blockIdx is the index of the block.

\begin{figure}
	\centering\includegraphics[width=4in]{figures/gpu_intro/CPUvsGPUwithMemcpy.eps}
	\caption{A comparison of CPU computation time verse GPU kernel computation time including memcpy from host to device and device to host.}
	\label{fig:CPUvsGPUwithMemcpy}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=4in]{figures/gpu_intro/CPUvsGPU.eps}
	\caption{A comparison of CPU computation time verse GPU kernel computation.}
	\label{fig:CPUvsGPU}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=4in]{figures/gpu_intro/CPUvsGPUzoomed.eps}
	\caption{A comparison of CPU computation time verse GPU kernel computation. Note: time is in $\mu$s.}
	\label{fig:CPUvsGPUzoomed}
\end{figure}




\section{NVIDIA Telsa GPU architecture}
A little bit of GPU architecture knowledge goes a long way.
This thesis uses NVIDIA Tesla K40c and K20c GPUs, Figure \ref{fig:GPUpicture} shows the form factor of the these GPUs.
NVIDIA's line of Tesla GPUs are built specifically for general purpose computing rather than dedicated graphics processing.
The red box in Figure \ref{fig:GPUarch} show the GPU chip and the yellow boxes show the SRAM that is \textit{off} the GPU chip.
\begin{figure}
	\centering\includegraphics[width=5in]{figures/gpu_intro/k40c_k20c.jpg}
	\caption{NVIDIA Tesla K40c and K20c.}
	\label{fig:GPUpicture}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=\textwidth]{figures/gpu_intro/Kepler_box.png}
	\caption{Example of an NVIDIA GPU card. The SRAM is shown to be boxed in yellow. The GPU chip is shown to be boxed in red.}
	\label{fig:GPUarch}
\end{figure}
\begin{table}
\begin{center}
\begin{tabular}{lll}
	\toprule
	Feature 			& Tesla K40c 	& Tesla K20c 	\\ \midrule
	Memory size (GDDR5) & 12 GB 		& 5 GB 			\\
	CUDA cores 			& 2880 			& 2496 			\\
	Base clock (MHz) 	& 745 			& 732 			\\ \bottomrule
\end{tabular}
\end{center}
\caption{The computational resources available with three NVIDIA GPUs used in this thesis (1x Tesla K40c 2x Tesla K20c).}
\label{tab:gpu-resources_jeffs}
\end{table}


Figure \ref{fig:GPU_blockDiagram} shows a block diagram of the Tesla architecture on the K20c and K40c GPUs.
The global memory and L2 memory are located in the SRAM off chip.
The GPU chip has thousands of relatively slow CUDA cores grouped together into streaming multiprocessors (SM).
GPU threads are assigned to SMs where they have access to registers, L1 cache and shared memory.
Table \ref{tab:gpu-resources_jeffs} summarizes the resources on the Tesla K40c and K20c.

The only memory that kernels have full control of is global memory and shared memory.
Data from host memory is transferred over the PCI Express bus to global memory.
Registers, L1 cache and L2 cache are handled and controlled by CUDA.
\begin{figure}
	\centering\includegraphics[width=5in]{figures/gpu_intro/GPU_blockDiagram.png}
	\caption{Block diagram of Tesla GPU microarchitecture. The dashed box indicates what blocks are located on the GPU chip. The dash dotted box indicates what blocks are located off the GPU chip.}
	\label{fig:GPU_blockDiagram}
\end{figure}

\section{CUDA programming}
A GPU cannot be programmed like a CPU because the architecture is totally different

When a kernel brings memory on chip from the off chip SRAM it takes 400-800 clock cycles.
Accessing the SRAM memory is inevitable but kernels should be designed to access SRAM memory as little as possible.



CUDA

Kernels

Threads

Libraries

GPU memory

Bad things to do in CUDA

Good things to do in CUDA

Massaging algorithms to map to a GPU well


%\section{memory access}
%	A nice linear ``coalesced'' memory access pattern is much better than a random access pattern. A GPU doesnt only grab one byte of memory at a time, it grabs 32 bytes, this is called a warp.
%	If thread n needs 4 bytes from memory, the GPU grabs that 4 bytes and the 28 bytes following the 4 bytes. Now, if thread n+1 needs the 4 bytes following the 4 bytes that thread n already got...thread n+1 doesnt have to go fetch the needed 4 bytes, because the GPU already got them.
%	If an algorithm has a strange access pattern, massaging the algorithm to have a linear access pattern can cause major speed ups.
%	GPUs are not computation bound, they are memory bandwidth bound.
%	Most accesses are to global memory
%	If you dont first optimize the global memory accesses, there is no point in doing other optimizations.
%	Global memory accesses take from 400-800 clock cycles 
%	If an algorithm wants to grab every 10 indicies, that kernel will be slow, but if the memory was reorganized so all the desired memory is coalesced or not strided, a major speed up will occur.	
%	
%\section{Things to avoid}
%Thread Divergence
%Thread Branching
%same code path for each thread
%Going to Global memory alot
%Inplace opperations that are dependent on other threads
%
%
%\section{Things to do}
%Massage algorithms so GPU librarys can be used.
%If libraries cant be used, write customs Kernels that have linear access patterns. (steps to get an algorithm ready for the GPU)
%Coellessing linear memory access
%Threads per block optimization
%Access Global Memory as little as possible
%Use shared memory if many threads use the same array.
%Program with warps in mind
%Optimize global memory access first, then do other optimizations.
%Have the CPU launch GPU kernels then do something. The CPU literally sit and waits for the GPU to finish.
%Trade work for steps (if you can decrease the number of steps by increasing the work...GPUs will go faster) (small little steps that can be massively parallel)
%
%
%
%\section{number of threads}
%	Kernels can speed up drastically when the optimal number of threads are used 
%	Show some plots of this
%	Sometimes memory access patters of algorithms cannot optimized perfectly, at this point we just turn to finding the optimal number of threads per block to launch.
%	If too many threads are launched, the GPU will be attepting to fetch too much memory. If not enough threads are launched, then the threads will be waiting for other threads to be done.
%	The number of threads should saturate the bus, but not request too much.
%	The optimum number of threads can be found experimentally. There is a finite number of threads per block that are possible (1:1024). So just build a for loop that runs the number of threads pre block from 1 to 1024 and time the kernel...which ever one is fastest is the optimum number of threads per block.
%	Most of the time this optimum number will be a multiple of 32 because of the warp or memory access architecture of the GPU.
%	No Thread divergence! Thread divergence is when a kernel has an if statment that causes Programming GPU Kernels should be written so every thread has the same structure or flow. 
%	
%\section{libraries}
%	NVIDIA builds CUDA libraries to do common operations like Basic Linear Algebra Subprograms (BLAS), Fast Fourier Transforms (FFT) and system solvers. These libraries were written and optimized by the NVIDIA Ninjas. Who can beat a Ninja? Right, no one can. So, when you can use a library, DO!
%	The Ninjas give the user no control over how many threads per block are launched, really the library looks like a serial library but the library kernels are massively parallel.
%	
%\section{Loop unrolling}
%	GPUs do not guarantee the thread n will run before thread n+1. So algorithms that are serial, or the $n$-th iteration depends on data from iteration n-1 do not map well to GPUs. In fact, the threads on GPUs are launched in no predictable or regular order.
%	To parallelize an algorithm that is based on a \textit{for} loop, each iteration of the \textit{for} loop must be completely independent of other iterations. To test if the \textit{for} loop is ready to be parallelized, run the indicies in a random order, is the result correct? Does the result change depending on the order of the indicies.
%	An algorithm can also be broken up into chunks that can be parallelized, like the FFT. Each stage of butter fly and twiddle factor can be broken up a single parallel kernel. 
%
%
%
%%\section{Overview}
%%
%%A Graphics Processing Unit (GPU) is a computational unit with a specialized, highly-parallel architecture well-suited to processing large blocks of data.
%%The performance advantage derives from the GPUs ability to perform a large number of parallel computations on the large data block.
%%Historically, the large block of data was graphics data and the computations were limited to those required for graphics operations.
%%Since about 2006, there has been increasing interest in using genral purpose GPUs for more general processing tasks such as machine learning, oil exploration, scientific image processing, linear algebra, statistics, and 3D reconstruction \cite{wikipedia-gpu:2015}.
%%This project leverages this trend and applies GPU processing to the estimation, computation, and filtering operations required for data-aided equalization.
%%
%%Typically, the GPU architecture comprises a large memory block accessible by several (up to a few thousand) processing units simultaneously.
%%For example, memory and processing units (called ``CUDA cores'') for the two NVIDIA GPUs used in this project are summarized in Table~\ref{tab:gpu-resources}.
%%Consequently, algorithms that are highly parallel are best suited for the GPU.
%%In constrast, sequential algorithms (e.g., a phase lock loop!) are not well suited for the GPU.
%%
%%Programming the NVIDIA GPUs is written in the C++ computer language with API extensions known as CUDA (Compute Unified Device Architecture).
%%The CUDA API is ``a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements'' \cite{wikipedia-cuda:2015}.
%%CUDA allows the C++ program, running on the host CPU, to launch special functions---known as kernels--- on the GPU while allowing some of the functionality to remain on the host CPU.
%%
%%The choice to use GPUs, rather than FPGAs, for this project include the following:
%%\begin{enumerate}
%%	\item The desire to test the performance of four equalizers operating in parallel on the
%%		received data is a good match to GPU structure.
%%	\item Programming in C++ has advantages over VHDL designs in an FPGA.
%%		Development time is much shorter and debugging is quite a bit easier.
%%		Small modifications are much easier with C++ and GPUs than with VHDL in and FPGA.
%%	\item  Because the GPU supports floating point operations, the complications of tracking
%%		``digit growth'' in fixed-point operations (on an FPGA) are eliminated.
%%\end{enumerate}
%%GPUs are unlikely to be used for telemetry demodulators in the foreseeable future.
%%However, because the point of the project is to assess the performance of competing algorithms,
%%this fact is less important.
%%Once identified, the ``best'' equalizer algorithm can be designed in VHDL and incorporated
%%into the FPGA-based demodulators commonly found on the telemetry market.
%%
%%The unique features of the GPU architecture require the designer to rethink how the
%%signal processing is organized.
%%DRAM memory limitations on the FPGA limit the number of samples per transfer $39{,}321{,}600$
%%complex-valued samples ($314{,}572{,}800$ Bytes).
%%This data, corresponding to $3{,}103$ packets,
%%is loaded into the GPU memory (cf. Table~\ref{tab:gpu-resources}).
%%Here starting indexes of the $3{,}103$ occurrences of the preamble are found.
%%Subsequent signal processing for each packet is performed \textit{in parallel}.
%%In the end, $3{,}103 \times 6{,}144 = 19{,}064{,}832$ data bits \textit{per equalizer}
%%are produced at the end of the processing applied to each block.
%%A conceptual block diagram of this organization is illustrated in Figure~\ref{fig:signal-flow-gpu}.
%%The preamble detector (or frame synchronizer), frequency offset estimator, channel estimator, and
%%noise variance estimator are described in Section~\ref{sec:estimators}.
%%The equalizers are described in Section~\ref{sec:eq}.
%%
%%
%%\begin{table}
%%\caption{The computational resources available with three NVIDIA GPUs used in this project (1x Tesla k40 2x Tesla K20).}
%%\label{tab:gpu-resources}
%%\begin{center}
%%\begin{tabular}{lll}
%%	\toprule
%%	Feature & Tesla k40 & Tesla K20 \\ \midrule
%%	Memory size (GDDR5) & 12 GB & 5 GB \\
%%	CUDA cores & 2880 & 2496 \\ \bottomrule
%%\end{tabular}
%%\end{center}
%%\end{table}
%%
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth]{figures/gpu/signal-flow-gpu-2}
%%	\caption{A block diagram illustrating organization of the algorithms in the GPU.}
%%	\label{fig:signal-flow-gpu}
%%\end{figure}
%
%%\section{GPU Algorithm Implementation}
%%Once the $39{,}321{,}600$ complex floating point received samples are transferred from the FPGA to the host CPU, they are transferred to the Tesla k40 GPU shown in Figure \ref{fig:paq-hardware-overview} in Chapter \ref{sec:hardware}.
%%With the samples tresferred to the k40 GPU, all the algorithms are ready to run.
%%Figure \ref{fig:software-algorithmBlockDiagramSimple} shows the steps in the PAQ GPU implementation. 
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/software-algorithmBlockDiagramSimple.png}
%%	\caption{A block diagram illustrating algorithm flow}
%%	\label{fig:software-algorithmBlockDiagramSimple}
%%\end{figure}
%%
%%The frame sync block in Figure \ref{fig:software-algorithmBlockDiagramSimple} uses a preamble detector to find the indices for the start of the $3,103$ packets in the received. 
%%The frame sync block also packetizes the data into $L_{pkt}$ portions of $12,672$ samples.
%%Before the the channel can be estimated, the frequency offset is removed because so the channel estimator is accurate.
%%The channel and noise are then estimated using the preamble.
%%With the preambles located, the frequency offset and the channel response and noise variance estimated, the 5 studied equalizers are calculated using the estimated channel and noise variance.
%%
%%\subsection{Convolution GPU Implementation}
%%A common tool in digital signal processing is the discrete time convolution sum.
%%A convolution sum applies a finite impulse response (FIR) filter to a signal.
%%The FIR filter might be low pass, high pass or an equalizer.
%%Applying a FIR filter $\mathbf{h}$ to a signal vector $\mathbf{x}$ is
%%\begin{equation}
%%y(n) = \left(\mathbf{x}*\mathbf{h}\right)(n) = \sum_{m=0}^{L_h-1} x(n) h(n-m)
%%\label{eq:time-convolution}
%%\end{equation}
%%when $\mathbf{h}$ is $L_h$ long with the center tap $n_0$. 
%%Figure \ref{fig:conv-n_0} shows the convolution of $\mathbf{x}$ with $\mathbf{h}$ results in $\mathbf{y}$.
%%The desired portion of $\mathbf{y}$ is $L_y$ long and pruned out starting at $n_0$ (the center tap index of $\mathbf{h}$).
%%The grey boxes in Figure \ref{fig:conv-n_0} are the pruned samples of $\mathbf{y}$.
%%
%%Convolution can also be expressed as the matrix operation
%%\begin{equation}
%%\begin{bmatrix}
%%y(0) \\
%%y(1) \\
%%\vdots \\
%%y(L_x-2) \\
%%y(L_x-1)
%%\end{bmatrix} = 
%%\begin{bmatrix}
%%x(n_0) & x(n_0-1) & \cdots & x(n_0-(L_h-1))\\
%%x(n_0+1) &x(n_0) & \cdots & x(n_0-L_h) \\
%%\vdots & \vdots &\ddots &\vdots\\
%%\\
%%  & & & x(n_0) \\
%%  & & & x(n_0+1) \\
%% \vdots & \vdots & & \vdots \\
%% x(L_x+n_0) & x(L_x+n_0+1) & \cdots & x(L_x+L_h+n_0-1)
%%\end{bmatrix}
%%\begin{bmatrix}
%%h(0)\\
%%h(1)\\
%%\vdots\\
%%h(L_h-1)
%%\end{bmatrix}
%%\label{eq:conv-matrix}
%%\end{equation} 
%%or
%%\begin{equation}
%%\mathbf{y} = \mathbf{X}\mathbf{h}.
%%\label{eq:matrix_conv}
%%\end{equation}
%%where the result $\mathbf{y}$ is the matrix multiplication of the $L_x \times L_h$ convolution matrix $\mathbf{X}$ and filter coefficients $\mathbf{h}$.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/conv-n_0.png}
%%	\caption{The convolution of the vector $\mathbf{x}$ with the vector $\mathbf{h}$ results 					in the vector $\mathbf{y}$. The grey desired portion of $\mathbf{y}$ is from 					$n_0$ to $n_0+L_y$.}
%%	\label{fig:conv-n_0}
%%\end{figure}
%%
%%With a large data set or filter, implementing convolution with Fast Fourier Transforms (FFTs) is computationally more efficient than using equation \eqref{eq:time-convolution} or \eqref{eq:matrix_conv}.
%%FFTs are used to compute a $N$ point Discrete Fourier Transform (DFT) quickly and efficiently.
%%\begin{equation}
%%\mathbf{x}*\mathbf{h} = \text{IFFT}_N\Big[\text{FFT}_N\left[\mathbf{x}\right]\cdot\text{FFT}_N\left[\mathbf{h}\right]\Big]
%%\label{eq:fft-conv}
%%\end{equation}
%%The FFT algorithm is most efficient when $N$ is a power of $2$: $2$, $4$, $8$, $16$, $32$, ect.
%%The FFT length $N$ must be greater or equal to $L_x + L_h - 1$.
%%Each vector, $\mathbf{x}$ and $\mathbf{h}$ must be zero padded to the length $N$.
%%
%%The block diagram in Figure \ref{fig:conv_FFT_block} shows how a convolution is implemented in a GPU using FFTs.
%%To implement the zero pad block, the GPU launches $L_x$ or $L_h$ threads to fill a zero initialized vector.
%%The FFT block is done with a cuFFT library to compute the forward FFT \cite{NVIDIA-CUDAdoc:2015}.
%%The FFTs of $\mathbf{x}$ and $\mathbf{h}$ are multiplied using a hadamard point to point multiply by launching $N$ GPU threads.
%%With the FFTs multiplied, the GPU calls an inverse FFT or IFFT from the cuFFT library.
%%The GPU then launches $L_y$ threads to prune out the desired portion of the IFFT.
%%\begin{figure}
%%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/conv_FFT_block.png}
%%	\caption{Block diagram showing how the convolution of two vectors can be done using FFTs.}
%%	\label{fig:conv_FFT_block}
%%\end{figure}
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/conv_FFT_block3.png}
%%	\caption{Block diagram showing how the convolution of three vectors can be done using FFTs.}
%%	\label{fig:conv_FFT_block3}
%%\end{figure}
%%
%%To convolove more than two vectors, another row of zero pad and FFT blocks must be added for each additional vector and $n_0$ is adjusted accordingly.
%%Figure \ref{fig:conv_FFT_block3} shows the block diagram from convolving three vectors $\mathbf{x}$, $\mathbf{h}$ and $\mathbf{d}$.
%%For simplicity and clarity only, the blocks in Figure \ref{fig:conv_simple_block} will be shown from now on instead of Figures \ref{fig:conv_FFT_block} or \ref{fig:conv_FFT_block3}.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*9]{figures/gpu/conv_simple_block.png}
%%	\caption{Instead of showing Figure \ref{fig:conv_FFT_block} or \ref{fig:conv_FFT_block3}, the single block convolution is shown.}
%%	\label{fig:conv_simple_block}
%%\end{figure}
%%
%%\subsection{Estimator GPU Implementation}
%%Before calculating equalizers to combat multipath the GPU must find each packet and synchronize the received samples to used the preamble and ASM.
%%The equalizers are calculated based on the channel and noise estimates.
%%The channel and noise estimators are sensitive to frequency offset.
%%The frequency offset must be removed if it is present in the received samples to have accurate estimates.
%
%%\subsubsection{Frame Synchronization}
%%To compute preamble assisted equalizers, estimators use the preamble to estimate various parameters. 
%%The iNET packet is comprised of three different sections: preamble, asynchronous marker (ASM) and the data. 
%%The packet and received sample structure is shown in Figure\ref{fig:packet}.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth]{figures/gpu/packet.png}
%%	\caption{The iNET packet structure.}
%%	\label{fig:packet}
%%\end{figure}
%%
%%The goal of the frame synchronization step is to synchronize the received samples into frames or packets by locating the preambles. 
%%To give an overview of the frame synchronization step, synchronization of the received samples is explained using a preamble detector and search algorithms to packetize the received samples. 
%%To find the preambles in the batch, a preamble detector is used to compute the sample correlation function between the received samples and the preamble. 
%%Search algorithms then search the correlation function for sample indices that correlate strongest to the preamble. These indices are the starting indices of each packet in the received samples. 
%%Finally, using the starting indices of each packet, received samples are structured and synchronized into frames or packets.
%%
%%The first step in the frame synchronizer is to compute the sample correlation between the received samples and the preamble.
%%A lower complexity preamble detector is shown in equation \eqref{eq:L-4}-\eqref{eq:L-pedone-geoghegan-4} in section \ref{sec:estimators} and repeated here for convenience.
%%\begin{equation}
%%	L(n) = \sum_{m=0}^{7}
%%		\left[ I^2(n,m) + Q^2(n,m) \right]
%%	\label{eq:gpu-L-4}
%%\end{equation}
%%where
%%\begin{multline}
%%	I(n,m) \approx \sum_{\ell\in\mathcal{L}_1}r_R(\ell+32m+n)
%%			- \sum_{\ell\in\mathcal{L}_2}r_R(\ell+32m+n)
%%			+ \sum_{\ell\in\mathcal{L}_3}r_I(\ell+32m+n)
%%			- \sum_{\ell\in\mathcal{L}_4}r_I(\ell+32m+n)
%%			\\
%%			+ 0.7071 \left[
%%				\sum_{\ell\in\mathcal{L}_5}r_R(\ell+32m+n)
%%				- \sum_{\ell\in\mathcal{L}_6}r_R(\ell+32m+n)
%%			\right. \\
%%			\left.
%%				+ \sum_{\ell\in\mathcal{L}_7}r_I(\ell+32m+n)
%%				- \sum_{\ell\in\mathcal{L}_8}r_I(\ell+32m+n)
%%			\right],
%%	\label{eq:gpu-L-pedone-geoghegan-2}
%%\end{multline}
%%\begin{multline}
%%	Q(n,m) \approx \sum_{\ell\in\mathcal{L}_1}r_I(\ell+32m+n)
%%			- \sum_{\ell\in\mathcal{L}_2}r_I(\ell+32m+n)
%%			\\
%%			- \sum_{\ell\in\mathcal{L}_3}r_R(\ell+32m+n)
%%			+ \sum_{\ell\in\mathcal{L}_4}r_R(\ell+32m+n)
%%			\\
%%			+ 0.7071 \left[
%%				\sum_{\ell\in\mathcal{L}_5}r_I(\ell+32m+n)
%%				- \sum_{\ell\in\mathcal{L}_6}r_I(\ell+32m+n)
%%			\right. \\
%%			\left.
%%				- \sum_{\ell\in\mathcal{L}_7}r_R(\ell+32m+n)
%%				+ \sum_{\ell\in\mathcal{L}_8}r_R(\ell+32m+n)
%%			\right]
%%		\label{eq:gpu-L-pedone-geoghegan-3}
%%\end{multline}
%%with
%%\begin{equation}
%%	\begin{split}
%%	\mathcal{L}_1 &= \{ 0, 8, 16, 24 \}\\
%%	\mathcal{L}_2 &= \{ 4, 20 \}\\
%%	\mathcal{L}_3 &= \{ 2, 10, 14, 22 \}\\
%%	\mathcal{L}_4 &= \{ 6, 18, 26, 30 \}\\
%%	\mathcal{L}_5 &= \{ 1, 7,  9, 15, 17, 23, 25, 31 \}\\
%%	\mathcal{L}_6 &= \{ 3, 5, 11, 12, 13, 19, 21, 27, 28, 29 \}\\
%%	\mathcal{L}_7 &= \{ 1, 3,  9, 11, 12, 13, 15, 21, 23 \}\\
%%	\mathcal{L}_8 &= \{ 5, 7, 17, 19, 25, 27, 28, 29, 31 \}.
%%\end{split}
%%\label{eq:gpu-L-pedone-geoghegan-4}
%%\end{equation}
%%
%%The preamble detector is implemented in the GPU in two kernels as shown by the dotted box in Figure \ref{fig:preambleBlock}.
%%The first kernel computes the inner summations and the second computes the outer summation.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/preambleBlock.png}
%%	\caption{The block diagram for the frame synchronization implementation.}
%%	\label{fig:preambleBlock}
%%\end{figure}
%%
%%The inner summation, as defined in equations \eqref{eq:gpu-L-pedone-geoghegan-2}-\eqref{eq:gpu-L-pedone-geoghegan-4}, computes an $I$ and $Q$ pair.
%%A single $I$ and $Q$ pair is computed by summing 32 scaled real or imaginary parts of received samples.
%%For each received sample an $I$ and $Q$ pair is computed.
%%
%%The outer summation, as defined in equation \eqref{eq:gpu-L-4}, computes the reduced complexity maximum likelihood correlation $L$.
%%One sample of $L$ is computed by summing 8 squared then summed $I$ and $Q$ pairs.
%%For each received sample $L$ is computed.
%%
%%Figure \ref{fig:L_2_packets} shows an example of the first $2\times L_{pkt} $ samples of $L$.
%%The local maximums in $L$ in the figure indicate a preamble starts at the maximum's sample index.
%%The first local maximum is at sample index $7040$, indicating the first packet starts at that index.
%%The figure also shows the second packet starts at sample index $19712$.
%%The difference between these sample indices is $ L_{pkt} $, this difference agrees with the packet length shown in Figure \ref{fig:packet}.
%%\begin{figure}
%%	\centering\includegraphics[width=5in]{figures/gpu/L_2_packets.eps}
%%	\caption{The output of the Preamble Detector $L(u)$.}
%%	\label{fig:L_2_packets}
%%\end{figure}
%%
%%Because of the structure of the preamble, the preamble detector output $L$ has some unique properties.
%%The Figures in \ref{fig:L_corr_creepy} show the correlation function around an expected preamble location.
%%The correlation functions have peaks every 32 samples because the preamble bit sequence comprises eight repetitions of the 16-bit pattern $\text{CD98}_\text{hex}$.
%%The repetitive structure causes one main correlation peak with seven side peaks.
%%
%%When ideal samples are received the preamble detector output looks like Figure \ref{fig:L_corr_creepy}(a).
%%The structure of the correlation peaks still occur when the signal to noise ratio is low, as shown in Figure \ref{fig:L_corr_creepy}(b).
%%But when the signal to noise ratio is low and major multipath distortion happen, the correlation peaks look like Figures \ref{fig:L_corr_creepy}(c) and (d).
%%The structure of the correlation peaks can cause a simple algorithm to find an incorrect preamble starting location.
%%
%%In the worst case scenario, a simple algorithm might find an incorrect preamble index by searching a poorly placed search window.
%%A poorly placed window might search the large side correlation peaks from Figure \ref{fig:L_corr_creepy}(a) and the small main correlation peaks from Figures \ref{fig:L_corr_creepy}(c) or (d).
%%Because the first three side peaks in Figure \ref{fig:L_corr_creepy}(a) are much taller than the main peak in Figures \ref{fig:L_corr_creepy}(c) and (d), an incorrect preamble starting indices will result if search windows are not defined safely.
%%\begin{figure}
%%	\begin{center}
%%		\begin{tabular}{cc}
%%			\begin{minipage}[c]{3in}
%%				\includegraphics[width=3in]{figures/gpu/L_corr_8_clean.eps}
%%			\end{minipage} 
%%			&  
%%			\begin{minipage}[c]{3in}
%%				\includegraphics[width=3in]{figures/gpu/L_corr_8_noise.eps}
%%			\end{minipage} \\[12pt]
%%			
%%			\quad\quad\quad\quad(a)
%%			&
%%			\quad\quad\quad\quad(b) \\[12pt]
%%			
%%			\begin{minipage}[c]{3in}
%%				\includegraphics[width=3in]{figures/gpu/L_corr_17_creepy.eps}
%%			\end{minipage} 
%%			&  
%%			\begin{minipage}[c]{3in}
%%				\includegraphics[width=3in]{figures/gpu/L_corr_18_creepy.eps}
%%			\end{minipage} \\[12pt]
%%			
%%			\quad\quad\quad\quad(c)
%%			&
%%			\quad\quad\quad\quad(d)
%%			
%%		\end{tabular}
%%	\end{center}
%%	\caption{Detailed view of $L(u)$. 
%%			(a): correlation peaks of a distortion free and noiseless signal; 
%%			(b): correlation peaks of a distortion free but noisy signal with $E_b/N_0 = 0$dB; 
%%			(c): correlation peaks of a distorted and noisy signal with $E_b/N_0 = 0$dB;
%%			(d): correlation peaks of a distorted and noisy signal with $E_b/N_0 = 0$dB}
%%	\label{fig:L_corr_creepy}
%%\end{figure}
%%
%%To prevent searching multiple preamble correlation peaks, search windows should only search the correlation peak of one preamble.
%%To ensure the correlation peaks from only one preamble is searched, windows of length $ L_{pkt} $ are centered on expected preamble starting locations.
%%Figure \ref{fig:L_windows} shows an example of safe search windows centered on expected preamble starting locations.
%%\begin{figure}
%%	\centering\includegraphics[width=5in]{figures/gpu/L_windows.eps}
%%	\caption{Safe search windows defined to search only one preamble correlation peak.}
%%	\label{fig:L_windows}
%%\end{figure}
%%
%%To define safe search windows, a rough estimate of the preamble starting locations is made.
%%The GPU launches a kernel with one thread to search for the maximum in the first $ L_{pkt} $ samples of the received samples.
%%The maximum index is saved as $\hat{i}_0$.
%%$\hat{i}_0$ is used to make a rough estimate for all the preamble staring locations by building a vector defined as
%%\begin{align}
%%\hat{\mathbf{i}}
%%=     
%%\begin{bmatrix}
%%\hat{i}_0 + 0\times L_{pkt}  			\\
%%\hat{i}_0 + 1\times L_{pkt}  		\\
%%\vdots			\\
%%\hat{i}_0 + 3102\times L_{pkt}  		\\
%%\hat{i}_0 + 3103\times L_{pkt}  		
%%\end{bmatrix}
%%\label{eq:preamble_det_i_hat}
%%\end{align}
%%
%%With rough estimates of where the preambles should be located, the GPU searches safe windows of $L$ for local maximums.
%%On local maximum is found in each search window by centering each window on elements from $\hat{\mathbf{i}}$.
%%The GPU launches one thread per search window to find the local maximum and save its sample index in $\hat{\mathbf{i}}$.
%%The vector $\hat{\mathbf{i}}$ has the sample indices for $3103$ local maximums that should be $ L_{pkt} $ samples apart.
%%
%%Because of noise and multipath, the sample indices in $\hat{\mathbf{i}}$ are not the ideal $ L_{pkt} $ samples.
%%The GPU launches one thread to search $\hat{\mathbf{i}}$ for the longest chain of perfectly spaced indices.
%%The modulo of the last index in the longest chain of indices spaced $ L_{pkt} $ samples apart is the best estimate for $\hat{i}_0$.
%%Once again, $\hat{i}_0$ is updated with the best estimated first preamble starting location.
%%The vector $\hat{\mathbf{i}}$ is also updated again as in equation \eqref{eq:preamble_det_i_hat}. 
%%
%%Now with the best estimate of the preamble starting locations, the received samples can be packetized and synchronized.
%%Figure \ref{fig:packets_in_batch} shows the relationship of $\hat{\mathbf{i}}$ and $\boldsymbol{r}$.
%%Using $\hat{\mathbf{i}}$, the GPU launches one thread per received sample to packetize $\boldsymbol{r}$ into $\boldsymbol{r}_p$ as shown in Figure \ref{fig:packetized}. The structure of $r_p$ majorly simplifies indexing and removes the need for $\hat{\mathbf{i}}$.
%%\begin{equation}
%%\boldsymbol{r}_p = 
%%\begin{bmatrix}
%%r(\hat{i}_0) 			\\
%%r(\hat{i}_0+1) 		\\
%%\vdots			\\
%%r(\hat{i}_0+12670)\\
%%r(\hat{i}_0+12671)\\
%%r(\hat{i}_1) 			\\
%%r(\hat{i}_1+1) 		\\
%%\vdots			\\
%%r(\hat{i}_{3103}+12670)\\
%%r(\hat{i}_{3103}+12671)\\
%%\end{bmatrix}
%%\end{equation}
%%
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/packets_in_batch.png}
%%	\caption{The starting sample index for each packet in the batch.}
%%	\label{fig:packets_in_batch}
%%\end{figure}
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/packetized.png}
%%	\caption{The packetized structure of the received signals after the frame synchronization step.}
%%	\label{fig:packetized}
%%\end{figure}
%%
%%The steps in Figure \ref{fig:preambleBlock} synchronize the received samples $r$ into $r_p$ by using a preamble detector.
%%The output of the preamble detector, $L$, is searched for local maximums.
%%Imperfect spacing of local maximums is fixed by finding the longest chain of perfectly spaced indices in $\hat{\mathbf{i}}$.
%%The received samples are then packetized based on the corrected preamble spacing.
%%
%%\clearpage
%%
%%\subsubsection{Frequency Offset Compensation}
%%\label{sec:jeffs_frequencyoffsetestimator}
%%As discussed in section \ref{sec:joint-ML-estimator}, all the estimators depend on atleast one of the other parameters.
%%Estimating the frequency offset using the periodic properties of the preamble gives an accurate estimate, even in multipath.
%%The frequency offset is estimated using the data-aided frequency offset estimator in equation \eqref{eq:ML-w-final3}.
%%To compensate for the frequency offset, the received samples are rotated by the negative of estimated frequency offset.
%%The data-aided frequency offset estimator equation is repeated here for convenience.
%%\begin{equation}
%%	\hat{\omega}_0 = \frac{1}{L_q} \arg\left\{ \sum_{n=i+2L_q}^{i+7L_q-1} r(n)r^\ast(n-L_q)\right\}
%%	\label{eq:jeff-ML-w-final3}
%%\end{equation}
%%where
%%$\hat{\omega}_0$ is the frequency offset estimate for a single packet.
%%The summation in equation \eqref{eq:jeff-ML-w-final3} can be reformulated in terms of an inner product
%%\begin{equation}
%%	\hat{\omega}_0 = \frac{1}{L_q} \arg\left\{ \mathbf{r}_{p1}^T \mathbf{r}^\ast_{p2}  \right\}.
%%	\label{eq:jeff-ML-w-final3_reformed}
%%\end{equation}
%%where 
%%\begin{align}
%%\mathbf{r}_{p1}
%%=     
%%\begin{bmatrix}
%%r_p(2L_q) 			\\
%%r_p(2L_q+1) 		\\
%%\vdots			\\
%%r_p(7L_q - 2)\\
%%r_p(7L_q - 1)
%%\end{bmatrix}
%%\label{eq:freq_offset_r1}
%%\end{align}
%%\begin{align}
%%\mathbf{r}^\ast_{p2}
%%=     
%%\begin{bmatrix}
%%r^\ast_p(L_q) 			\\
%%r^\ast_p(L_q+1) 		\\
%%\vdots			\\
%%r^\ast_p(6L_q - 2)\\
%%r^\ast_p(6L_q - 1)
%%\end{bmatrix}
%%\end{align}
%%
%%Reformulating the estimator into an inner product isn't only simpler, it is also much faster in the GPU.
%%The GPU uses an extremely fast and efficient CUDA Basic Linear Algebra Subprogram (CUBLAS) library to compute the inner product \cite{NVIDIA-CUDAdoc:2015}.
%%Figure \ref{fig:frequencyOffsetBlock} shows how equation \eqref{eq:jeff-ML-w-final3_reformed} is implemented in GPU kernels.
%%The GPU strips $\mathbf{r}_{p1}$ and $\mathbf{r}^\ast_{p2}$ from the received packetized samples by launching one thread per packetized sample.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/frequencyOffsetBlock.png}
%%	\caption{The block diagram for the Frequency Offset estimator and sample rotation implementation.}
%%	\label{fig:frequencyOffsetBlock}
%%\end{figure}
%%
%%The GPU computes the inner product of $\mathbf{r}_{p1}$ and $\mathbf{r}^\ast_{p2}$ using a batched CUBLAS operation.
%%The arg and scale GPU kernel launches one thread per packet to compute the scaled angle of the inner product for each packet.
%%The scaled angle of each packet is saved in the vector $\hat{\boldsymbol{\omega}}_0$.
%%
%%In aeronautical telemetry, the frequency offset can be assumed to be constant over two seconds.
%%Because of this assumption, one frequency offset estimate is obtained by averaging all the offsets in a batch.
%%The GPU launches one thread to compute the average of the vector $\hat{\boldsymbol{\omega}}_0$ and save the average in the scalar value $\hat{\omega}^\prime_0$.
%%
%%The averaged frequency offset $\hat{\omega}^\prime_0$ is used to compensate for the frequency offset.
%%The frequency offset compensation can be done with the matrix $\boldsymbol{\Omega}_r$ the rotations due to the frequency offset are in matrix notation using 
%%\begin{equation}
%%	\boldsymbol{r}_r = \boldsymbol{\Omega}_r \boldsymbol{r}_p = 
%%	\begin{bmatrix} e^{-j\hat{\omega}^\prime_0(0)} & & \\ & \ddots & \\ & & e^{-j\hat{\omega}^\prime_0(3103\times L_{pkt} -1)} \end{bmatrix}
%%\begin{bmatrix}
%%r_p(0) 			\\
%%\vdots			\\
%%r_p(3103\times L_{pkt} -1)
%%\end{bmatrix}.
%%	\label{eq:jeff-Omega_0-matrix1}
%%\end{equation}
%%
%%Implementing equation \eqref{eq:jeff-Omega_0-matrix1} in GPUs would use an extreme amount of memory and computation because of the size and dimension of $\boldsymbol{\Omega}_r$.
%%Instead, the GPU launches one thread per packetized sample to rotate the packetized sample by $-\hat{\omega}^\prime_0$.
%%\begin{equation}
%%r_r(n) = r_p(n) \times e^{-j \hat{\omega}^\prime_0 n}
%%\end{equation}
%%
%%With the frequency offset compensation done, the samples are transferred over the PCIe bus from the Tesla k40 GPU to the two Tesla k20 GPUs, shown as the last block in Figure \ref{fig:frequencyOffsetBlock}.
%%
%%
%%\subsubsection{Channel Estimation}
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/channelBlock.png}
%%	\caption{The block diagram for the Channel estimator.}
%%	\label{fig:channelBlock}
%%\end{figure}
%%As mentioned in \ref{sec:joint-ML-estimator}, the channel cannot be estimated with a frequency offset in the received samples.
%%With the offset removed, the channel can accurately be estimated.
%%Equation \eqref{eq:ML-h-final2} is used to compute the channel estimate.
%%The equation is repeated below for convenience.
%%\begin{equation}
%%	\hat{\mathbf{h}} = \left( \mathbf{X}^\dag\mathbf{X} \right)^{-1} \mathbf{X}^\dag\hat{\boldsymbol{\Omega}}_0^\dag \mathbf{r}
%%\label{eq:jeff-ML-h-final2}
%%\end{equation}
%%
%%The channel estimator was originally formulated assuming the received samples still had a frequency offset.
%%The samples in the GPU have already compensated for the offset and \eqref{eq:jeff-ML-h-final2} is reformed into
%%\begin{equation}
%%	\hat{\mathbf{h}} = \left( \mathbf{X}^\dag\mathbf{X} \right)^{-1} \mathbf{X}^\dag \mathbf{r}_{r1}.
%%\label{eq:channel_reformed_no_freq}
%%\end{equation}
%%where
%%\begin{align}
%%\mathbf{r}_{r1}
%%=     
%%\begin{bmatrix}
%%r_r(N_2) 		\\
%%r_r(N_2+1) 		\\
%%\vdots			\\
%%r_r(L_p - N_1 - 2)\\
%%r_r(L_p - N_1 - 1)
%%\end{bmatrix}
%%\end{align}
%%
%%The matrix $\mathbf{X}$ is the $(L_p+L_a-N_1-N_2)\times (N_1+N_2+1)$ convolution matrix
%%formed from the iNet preamble and ASM samples. 
%%The psnudo inverse of $\mathbf{X}$, $\left( \mathbf{X}^\dag\mathbf{X} \right)^{-1} \mathbf{X}^\dag$ is computed once and saved as $\mathbf{X}_{pi}$.
%%Equipped with $\mathbf{X}_{pi}$ the channel estimator can be reformed again into
%%\begin{equation}
%%	\hat{\mathbf{h}} = \mathbf{X}_{pi} \mathbf{r}_{r1}.
%%\label{eq:channel_reformed_final}
%%\end{equation}
%%
%%The GPU only needs compute a matrix operation after stripping a portion of $\mathbf{r}_r$.
%%The GPU launches $L_p-N_1-N_2$ threads per packet strip $\mathbf{r}_{r1}$ from $\mathbf{r}_r$.
%%With $\mathbf{r}_{r1}$ built, the CUBLAS library is used to compute the matrix multiply to estimate the channel $\hat{\mathbf{h}}$.
%%The channel estimates are then transferred from the k40 GPU to the k20 GPUs.
%%Figure \ref{fig:channelBlock} shows the block diagram of how equation \eqref{eq:channel_reformed_final} is implemented in the GPU.
%%
%%\subsubsection{Noise Variance Estimation}
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/noiseBlock.png}
%%	\caption{The block diagram for the Noise Variance estimator.}
%%	\label{fig:noiseBlock}
%%\end{figure}
%%With the frequency offset removed and the channel estimated, the noise variance can be estimated.
%%The noise variance was studied in section \ref{sec:noise-variance} and \ref{sec:joint-ML-estimator}.
%%Equation \eqref{eq:ML-s2-final3} is the final form of the noise variance estimator and repeated here for convenience.
%%
%%\begin{equation}
%%	\hat{\sigma}_w^2 = \frac{1}{2\rho} \left| \mathbf{r}-\hat{\boldsymbol{\Omega}}_0\mathbf{X}\hat{\mathbf{h}}\right|^2
%%	\label{eq:jeff-ML-s2-final3}
%%\end{equation}
%%where
%%\begin{equation}
%%	\rho = {\rm Trace} \left\{ \mathbf{I} -  \mathbf{X}\left(\mathbf{X}^\dag\mathbf{X}\right)^{-1}\mathbf{X}^\dag \right\}.
%%\end{equation}
%%
%%Once again, $\boldsymbol{\Omega}_0$ is included in \eqref{eq:jeff-ML-s2-final3} assuming the vector $\mathbf{r}$ has a frequency offset.
%%The vector $\mathbf{r}_{r1}$ are received samples with the frequency offset compensated for, $\mathbf{r}_{r1}$ is the same vector from the channel estimator.
%%The noise variance estimator is reformulated replacing $\mathbf{r}$ and removing $\boldsymbol{\Omega}_0$.
%%\begin{equation}
%%	\hat{\sigma}_w^2 = \frac{1}{2\rho} \left| \mathbf{r}_{r1}-\mathbf{X}\hat{\mathbf{h}}\right|^2.
%%	\label{eq:noise_var_reformed}
%%\end{equation}
%%
%%To begin estimating the noise variance, the GPU first computes the matrix multiplication $\mathbf{X}\hat{\mathbf{h}}$.
%%The stored matrix $\mathbf{X}$ is a $(L_p+L_a-N_1-N_2)\times (N_1+N_2+1)$ convolution matrix and the channel estimate $\hat{\mathbf{h}}$ is a $(N_1+N_2+1)\times1$ vector.
%%The result of the matrix product $\mathbf{r}_m$ is simply the ideal preamble distorted by the estimated multipath channel.
%%The GPU calls a CUBLAS function to compute $\mathbf{r}_m$ quickly and efficiently.
%%
%%With $\mathbf{r}_m$ computed, $\frac{1}{2\rho} \left| \mathbf{r}_{r1}-\mathbf{r}_m\right|^2$ is split into two GPU kernels.
%%The first GPU kernel computes $\mathbf{s}^2$, an element by element magnitude squared difference vector of $\mathbf{r}_{m}$ and $\mathbf{r}_{r1}$ with $L_p+L_a-N_1-N_2$ threads for each packet.
%%\begin{equation}
%%s^2(n) = \left| r_{r1}(n)- r_m(n) \right|^2
%%\end{equation}
%%
%%For each packet, $\hat{\sigma}_w^2$ is computed by scaling the summation of by $\frac{1}{2\rho}$ in one GPU kernel.
%%The GPU kernel is launched with one thread per packet to scale a summation of $\mathbf{s}^2$ by $\nicefrac{1}{2\rho}$.
%%$\rho$ is a precomputed scalar based on the convolution matrix $\mathbf{X}$.
%%With the noise variance estimated for each packet, the estimates are transferred from the k40 GPU to the k20 GPUs.
%%These kernels are shown in Figure \ref{fig:noiseBlock}.
%%
%%
%%\subsection{Equalizer GPU Implementation}
%%
%%
%%\subsubsection{Zero-Forcing Equalizer}
%%The Zero-Forcing (ZF) equalizer was explored in section \ref{sec:zf}.
%%Equation \eqref{eq:spike-solution} computes the ZF filter coefficients.
%%The equation is repeated here for convenience with an added subscript.
%%\begin{equation}
%%	\mathbf{c}_{ZF} = \left( \mathbf{H}^\dag\mathbf{H}\right)^{-1}\mathbf{H}^\dag\mathbf{u}_{n_0}
%%	\label{eq:jeff-spike-solution}
%%\end{equation}
%%where $\mathbf{H}$ is the $(N_1+N_2+L_1+L_2+1)\times (L_1+L_2+1)$
%%convolution matrix formed by the channel impulse response $\hat{\mathbf{h}}$:
%%\begin{equation}
%%	\mathbf{H} = \begin{bmatrix}
%%		\hat{h}(-N_1) 	&  &  & \\
%%		\hat{h}(-N_1+1) & \hat{h}(-N_1) & & \\
%%		\vdots & \vdots & \ddots & \\
%%		\hat{h}(N_2) 	& \hat{h}(N_2-1) & & \hat{h}(-N_1)\\
%%		       & \hat{h}(N_2) & & \hat{h}(-N_1+1) \\
%%		       &        & & \vdots \\
%%		       &        & & \hat{h}(N_2)
%%		\end{bmatrix}.
%%\label{eq:jeff-zf-H-matrix}
%%\end{equation}
%%The length $L_1+L_2+1$ is the length of the equalizer $L_{eq}$.
%%The length $N_1+B_2$ is the length of the channel $L_ch$.
%%and we assume $c(n)$ has support on $-L_1 \leq n \leq L_2$ and that $\hat{h}(n)$
%%has support on $-N_1 \leq n \leq N_2$.
%%$\mathbf{u}_{n_0}$ is the $(N_1+N_2+L_1+L_2+1) \times 1$
%%vector representing the desired composite impulse response:
%%that is, a vector comprising all-zeros with a one in position $n_0$.
%%These vectors are
%%\begin{equation}
%%	\mathbf{c}_\text{ZF} = \begin{bmatrix} c(-L_1) \\ \vdots \\ c(0) \\ \vdots \\ c(L_2) \end{bmatrix}
%%	\qquad
%%	\mathbf{u}_{n_0} = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}
%%	\begin{matrix*}[l] \left. \vphantom{\begin{matrix} 0 \\ \vdots \\ 0 \end{matrix}} \right\}
%%		\text{$n_0-1$ zeros}
%%		\\ \\
%%		\left. \vphantom{\begin{matrix} 0 \\ \vdots \\ 0 \end{matrix}} \right\}
%%		\text{$N_1+N_2+L_1+L_2-n_0+1$ zeros}
%%		\end{matrix*}
%%\label{eq:jeff-cu-vector-def}
%%\end{equation}
%%
%%In equation \eqref{eq:jeff-spike-solution}, $\mathbf{H}^\dag\mathbf{H}$ is of the form
%%\begin{equation}
%%\mathbf{R}_{h_\text{ZF}} = \mathbf{H}^\dag\mathbf{H} = 
%%	\begin{bmatrix}
%%		r_{h_\text{ZF}}(0) 				& r_{h_\text{ZF}}(-1) 			& \cdots & r_{h_\text{ZF}}(-L_1-L_2)	\\
%%		r_{h_\text{ZF}}(1) 		& r_{h_\text{ZF}}(0) 			& \cdots & r_{h_\text{ZF}}(-L_1-L_2+1)		\\
%%		\vdots 				& \vdots 			& \ddots & \vdots  			\\
%%		\\
%%		r_{h_\text{ZF}}(L_1+L_2) & r_{h_\text{ZF}}(L_1+L_2)	& \cdots & r_{h_\text{ZF}}(0)  			\\
%%	\end{bmatrix}
%%	\label{eq:hhh}
%%\end{equation}
%%where $r_{h_\text{ZF}}(k)$ is the auto-correlation of the estimated channel
%%\begin{equation}
%%r_{h_\text{ZF}}(k) = \sum_{n=-N_1}^{N_2} \hat{h}(n)\hat{h}^\ast(n-k).
%%\end{equation}
%%The definition of $\mathbf{R}_{h_\text{ZF}}$, requires $r_{h_\text{ZF}}(k)$ for $-L_1-L_2 \leq n \leq L_1+L_2$. 
%%But $r_{h_\text{ZF}}(k)$ only has support on $-N_1-N_2 \leq n \leq N_1+N_2$ resulting in $r_{h_\text{ZF}}(k)$ being sparse which means $r_{h_\text{ZF}}(k)$ is mostly zero. 
%%In \eqref{eq:hhh}, $r_{h_\text{ZF}}(k)$ for $\left|k\right| > N_1+N_2$ is zero resulting in $\mathbf{R}_{h_\text{ZF}}$ also being sparse. 
%%In fact, $\mathbf{R}_{h_\text{ZF}}$ is $\% 64$ zeros.
%%
%%In equation \eqref{eq:jeff-spike-solution}, $\mathbf{H}^\dag\mathbf{u}_{n_0}$ can be simplified to the vector $\hat{\mathbf{h}}_{n_0}$
%%\begin{equation}
%%	\mathbf{H}^\dag\mathbf{u}_{n_0} = \hat{\mathbf{h}}_{n_0} = \begin{bmatrix} 0 \\ \vdots \\ 0 \\ \mathbf{h} \\ 0 \\ \vdots \\ 0 \end{bmatrix}
%%	\begin{matrix*}[l] \left. \vphantom{\begin{matrix} 0 \\ \vdots \\ 0 \end{matrix}} \right\}
%%		\text{$n_0-N_1-1$ zeros}
%%		\\ \\
%%		\left. \vphantom{\begin{matrix} 0 \\ \vdots \\ 0 \end{matrix}} \right\}
%%		\text{$N_1+L_1+L_2-n_0+1$ zeros.}
%%		\end{matrix*}
%%\label{eq:hun0-vector-def}
%%\end{equation}
%%where $\hat{\mathbf{h}}_{n_0}$ is just a zero padded version of the channel estimate $\mathbf{h}$.
%%
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/ZFblock.png}
%%	\caption{The block diagram for the Zero Forcing equalizer implementation.}
%%	\label{fig:ZFblock}
%%\end{figure}
%%
%%The computation of $\mathbf{c}_\text{ZF}$ looks fairly straight forward but the inverse of $\mathbf{H}^\dag\mathbf{H}$ presents a very challenging problem in GPU implementation.
%%GPUs perform extremely well on parallel algorithms but an inverse is inherently serial.
%%If the inverse was to implemented, the computation takes longer than $1.9065$ seconds.
%%Equation \eqref{eq:jeff-spike-solution} must be reformed to remove the inverse of $\mathbf{H}^\dag\mathbf{H}$.
%%\begin{equation}
%%	\mathbf{H}^\dag\mathbf{H}\mathbf{c}_\text{ZF} = \mathbf{H}^\dag\mathbf{u}_{n_0}
%%	\label{eq:jeff-spike-solution-reformed}
%%\end{equation}
%%or
%%\begin{equation}
%%	\mathbf{R}_{h_\text{ZF}} \mathbf{c}_\text{ZF} = \hat{\mathbf{h}}_{n_0}
%%	\label{eq:jeff-spike-solution-reformed_vectors}
%%\end{equation}
%%
%%
%%To eliminate the need for an inverse, $\mathbf{H}^\dag\mathbf{H}$ is moved to the left side of equation \eqref{eq:jeff-spike-solution}. Solving for $\mathbf{c}_\text{ZF}$ in \eqref{eq:jeff-spike-solution-reformed_vectors} can be done many ways but the sparseness of $\mathbf{R}_{h_\text{ZF}}$ must be leveraged to ensure the coeffiencts can be calcualted with in the $1.9065$ seconds. 
%%NVIDIA has a batched sparse solver library that computes $3103$ $\mathbf{c}_\text{ZF}$ equalizers in less than $410$ms.
%%
%%Figure \ref{fig:ZFblock} shows the ZF filter calculations only require the channel estimate $\hat{\mathbf{h}}$.
%%The GPU launches a kernel with $2(N_1+N_2)+1$ threads per packet to calculate the estimated channel auto-correlation $\mathbf{R}_{h_\text{ZF}}$.
%%The matrix $\mathbf{R}_{h_\text{ZF}}$ is built by the GPU with $\mathbf{R}_{h_\text{ZF}}$ by launching a kernel with a thread for every non-zero index in $\mathbf{R}_{h_\text{ZF}}$.
%%The GPU also builds the vector $\hat{\mathbf{h}}_{n_0}$ by zero padding the channel estimate with $N_1+N_2+1$ threads per packet.
%%The Zero forcing equalizer coefficients are then calculated using the batched sparse solver from the CUDA libary.
%%
%%
%%
%%\clearpage
%%\subsubsection{MMSE Equalizer}
%%The general formulation for the MMSE equalizer is the same as ZF equalizer.
%%The MMSE equalizer was explored in section \ref{sec:mmse}.
%%Equation \eqref{eq:copt-Ri} computes the MMSE filter coefficients.
%%The equation is repeated here for convenience with an added subscript.
%%The MMSE optimum equalizer filter coefficients are given by
%%\begin{equation}
%%	\mathbf{c}_\text{MMSE} = \left[ \vphantom{\sum} \mathbf{H}^\dag\mathbf{H} + %\frac{1}{2\nicefrac{E_b}{N_0}}
%%	\frac{2\sigma^2_w}{\sigma^2_s}
%%	\mathbf{I}_{L_1+L_2+1} \right]^{-1}
%%	\mathbf{g}^\dag
%%	\label{eq:jeff-copt-Ri}
%%\end{equation}
%%where $\mathbf{H}$ is the same $\mathbf{H}$ from the ZF equalizer in equation \eqref{eq:jeff-zf-H-matrix} and $\mathbf{I}_{L_1+L_2+1}$ is the $(L_1+L_2+1)\times(L_1+L_2+1)$ identity matrix.
%%The vector $\mathbf{g}^\dag$ is also the same as the vector $\hat{\mathbf{h}}_{n_0}$ in the ZF equalizer in equation \eqref{eq:hun0-vector-def}.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/MMSEblock.png}
%%	\caption{The block diagram for the MMSE equalizer implementation.}
%%	\label{fig:MMSEblock}
%%\end{figure}
%%The matrix operations in \eqref{eq:jeff-copt-Ri} can be expressed as
%%\begin{equation}
%%\left[\vphantom{\sum} \mathbf{H}^\dag\mathbf{H} + %\frac{1}{2\nicefrac{E_b}{N_0}}
%%	\frac{2\sigma^2_w}{\sigma^2_s}
%%	\mathbf{I}_{L_1+L_2+1}\right] = \mathbf{R}_{h_\text{MMSE}}
%%\end{equation}
%%where
%%\begin{equation}
%%\mathbf{R}_{h_\text{MMSE}} = 
%%		\begin{bmatrix}
%%		r_{h_\text{MMSE}}(0)				 & r_{h_\text{MMSE}}(-1) 			& \cdots & r_{h_\text{MMSE}}(-L_1-L_2)	\\
%%		r_{h_\text{MMSE}}(1) 		& r_{h_\text{MMSE}}(0)& \cdots & r_{h_\text{MMSE}}(-L_1-L_2+1)		\\
%%		\vdots 				& \vdots 			& \ddots & \vdots  			\\
%%		\\
%%		r_{h_\text{MMSE}}(L_1+L_2) & r_{h_\text{MMSE}}(L_1+L_2)	& \cdots & r_{h_\text{MMSE}}(0)		\\
%%	\end{bmatrix}
%%\end{equation}
%%and
%%\begin{equation}
%%r_{h_\text{MMSE}}(k) = \sum_{n=-N_1}^{N_2} \hat{h}(n)\hat{h}^\ast(n-k) + \hat{\sigma}_w^2 \delta(k).
%%\end{equation}
%%
%%The MMSE equalizer coeffiencts are calculated almost the same way as the ZF equalizer.
%%The only little difference is calculating $r_{h_\text{MMSE}}(k)$, the noise variance $\hat{\sigma}_w^2$ is added to $r_{h_\text{MMSE}}(0)$. 
%%Aside from adding $\hat{\sigma}_w^2$, the GPU implements 
%%\begin{equation}
%%	\mathbf{R}_{h_\text{MMSE}} \mathbf{c}_\text{MMSE} = \hat{\mathbf{h}}_{n_0}
%%	\label{eq:jeff-ZF-vectors}
%%\end{equation}
%%exactly the same way as equation \eqref{eq:jeff-spike-solution-reformed_vectors}. 
%%The MMSE block diagram in Figure \ref{fig:MMSEblock} is the same as the ZF block diagram in Figure \ref{fig:ZFblock} aside from the noise variance going into the auto-corr block.
%%
%%\subsubsection{CMA Equalizer}
%%The constant modulus algorithm (CMA) equalizer was explored in section \ref{sec:CMA}.
%%Equation \eqref{eq:cma-update-summary} updates the CMA filter coefficients using Equation \ref{eq:DelJcma-approx} to compute $\nabla J_\text{CMA}$.
%%The equations are repeated here for convenience with a few changes.
%%\begin{equation}
%%	\nabla J_\text{CMA} \approx \frac{2}{2N_b} \sum_{n=0}^{2N_b-1}
%%	\left[ \vphantom{\displaystyle\sum}  y(n) y^\ast(n) - R_2 \right]
%%	y(n)  \mathbf{r}^\ast(n).
%%\label{eq:jeff-DelJcma-approx}
%%\end{equation}
%%\begin{equation}
%%	\mathbf{c}_{b+1} = \mathbf{c}_b - \mu \nabla J_\text{CMA}
%%\label{eq:jeff-cma-update-summary}
%%\end{equation}
%%In section \ref{sec:CMA} equation \eqref{eq:jeff-DelJcma-approx} was derived generally, but here we now have known data and equalizer lengths.
%%There is only one $\nabla \mathbf{J}_\text{CMA}$ so the subscript CMA is dropped.
%%The adjustment $\nabla \mathbf{J}$ is the same length $L_{eq}$ as the equalizer $\mathbf{c}_n$.
%%The portion of $\mathbf{r}^\ast(k)$ starts at the center tap of the equalizer $L_1$.
%%The block index does not need to be tracked because of the frame synconization step, so the subscript $b+1$ and $b$ is replaced with the iteration index $n$ and $n+1$.
%%Because the index $n$ is being used as the iteration index, the summation index is replaced with $k$.
%%Applying all of these changes results in 
%%\begin{equation}
%%	\nabla \mathbf{J} \approx \sum_{k=0}^{L_{pkt}-1} 
%%	z(k) \mathbf{r}^\ast(k).
%%\label{eq:jeff-DelJcma-approx}
%%\end{equation}
%%\begin{equation}
%%	\mathbf{c}_{n+1} = \mathbf{c}_n - \mu \nabla \mathbf{J}
%%\label{eq:jeff-cma-update-summary}
%%\end{equation}
%%where
%%\begin{equation}
%%z(k) = \left[ \vphantom{\displaystyle\sum}  y(k) y^\ast(k) - R_2 \right]
%%	y(k).
%%\end{equation}
%%
%%The computation of $\nabla \mathbf{J}$ can be done directly by launching $L_{eq}$ threads to compute a length $L_{pkt}$ summation per thread.
%%But a long summation inside a GPU thread is terribly inefficient.
%%The equation for $\nabla \mathbf{J}$ can be reformulated or massaged to look like a convolutional sum.
%%This will be shown by by doing a toy example.
%%
%%Suppose that the vectors $\mathbf{r}^\ast$ and $\mathbf{z}$ are length $4$ and zero outside of $0 \leq k \leq 3$. 
%%The desired vector $\nabla \mathbf{J}$ is $3$ long and $L_1$ is $1$.
%%The toy example computation of $\nabla \mathbf{J}$ is
%%\begin{align}
%%\nabla J(0) &= z(0)r(1)				+z(1)r(2) +z(2)r(3)				\\ \nonumber
%%\nabla J(1) &= z(0)r(0)				+z(1)r(1) +z(2)r(2)	+z(3)r(3)	\\ \nonumber
%%\nabla J(2) &= \indent\indent\indent\;  z(1)r(0) +z(2)r(1) +z(3)r(2).	\\ \nonumber
%%\label{eq:toy-delj}
%%\end{align}
%%A convolutional sum of the vector $\mathbf{z}$ with $\mathbf{r}$ is
%%\begin{align}
%%y(0) &= z(0)r(0)																		\\\nonumber
%%y(1) &= z(0)r(1) 				+z(1)r(0)													\\ \nonumber
%%y(2) &= z(0)r(2) 				+z(1)r(1) 				+z(2)r(0)							\\ \nonumber
%%y(3) &= z(0)r(3) 				+z(1)r(2) 				+z(2)r(1) 				+z(3)r(0) 	\\ \nonumber
%%y(4) &= \indent\indent\indent\;  z(1)r(3) 				+z(2)r(2) 				+z(3)r(1) \\ \nonumber
%%y(5) &= \indent\indent\indent\indent\indent\indent\>\,\, z(2)r(3) 				+z(3)r(2) \\ \nonumber
%%y(6) &= \indent\indent\indent\indent\indent\indent\indent\indent\indent\;\;\;		 z(3)r(3) \\ \nonumber
%%\label{eq:toy-conv}
%%\end{align}
%%The convolutional sum looks kind of like the $\nabla \mathbf{J}$ but longer.
%%The desired section of $\mathbf{y}$ is $2 \leq k \leq 4$.
%%\begin{align}
%%y(2) &= z(0)r(2) 				+z(1)r(1) 				+z(2)r(0)							\\ \nonumber
%%y(3) &= z(0)r(3) 				+z(1)r(2) 				+z(2)r(1) 				+z(3)r(0) 	\\ \nonumber
%%y(4) &= \indent\indent\indent\;  z(1)r(3) 				+z(2)r(2) 				+z(3)r(1)  \nonumber
%%\label{eq:y_portion}
%%\end{align}
%%The set of convolutional equations are structured like $\nabla \mathbf{J}$ but the indices in $r(n)$ are ascending in Equations \eqref{eq:toy-delj} but descending in Equations \eqref{eq:y_portion}.
%%One of the input vectors $\mathbf{z}$ or $\mathbf{r}$ need to be reversed.
%%
%%\clearpage
%%Now the indicies are moving in the right directions but the convolutional sum has the wrong structure.
%%The vector $\mathbf{y}$ needs to be reversed also.
%%\begin{align}
%%y(4) &= z(0)r(1)				+z(1)r(2) +z(2)r(3)				\\ \nonumber
%%y(3) &= z(0)r(0)				+z(1)r(1) +z(2)r(2)	+z(3)r(3)	\\ \nonumber
%%y(2) &= \indent\indent\indent\;  z(1)r(0) +z(2)r(1) +z(3)r(2).	\\ \nonumber
%%\label{eq:y_reverse}
%%\end{align}
%%Now the convolutional sum $\mathbf{y}$ has been massaged to look just like $\nabla \mathbf{J}$.
%%\begin{align}
%%\nabla J(0) &= z(0)r(1)				+z(1)r(2) +z(2)r(3)				\\ \nonumber
%%\nabla J(1) &= z(0)r(0)				+z(1)r(1) +z(2)r(2)	+z(3)r(3)	\\ \nonumber
%%\nabla J(2) &= \indent\indent\indent\;  z(1)r(0) +z(2)r(1) +z(3)r(2).	\\ \nonumber
%%\label{eq:toy-delj_repeat}
%%\end{align}
%%
%%This Toy example shows that a simple convolution can be used to calculate $\nabla \mathbf{J}$ instead of a long summation for each index of the gradient.
%%Figure \ref{fig:delJ_convORsum} shows how to use a convolution to compute $\nabla \mathbf{J}$.
%%One of the input vectors and the output vector must be reversed to compute $\nabla \mathbf{J}$ using a convolution.
%%The $L_{eq}$ long portion of the convolution starts at the index $L_{pkt} - L_2 - 1$.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/delJ_convORsum.png}
%%	\caption{The computation of $\nabla \mathbf{J}$ can be done with a convolution of directly.}
%%	\label{fig:delJ_convORsum}
%%\end{figure}
%%The block diagram in Figure \ref{fig:CMA_block} shows how the GPU implements the CMA algorithm.
%%First the past filter $\mathbf{c}_n$ is applied by a convolution.
%%The GPU computes $\mathbf{z}$ using $L_{pkt}$ threads per packet, then reverses the vector in preparation for the convolution.
%%The convolution is done as shown in Figure \ref{fig:conv_FFT_block}.
%%The GPU then reverses the output vector to obtain $\nabla \mathbf{J}$ from the convolution.
%%With $\nabla \mathbf{J}$ computed, the next CMA filter coefficients $\mathbf{c}_{n+1}$ are updated using $L_{eq}$ threads per packet.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/CMA_block.png}
%%	\caption{The block diagram showing how the GPU implements the CMA algorithm.}
%%	\label{fig:CMA_block}
%%\end{figure}
%%
%%
%%\subsubsection{Frequency Domain Equalizer 1}
%%The Frequency Domain Equalizer 1 (FDE1) was studied in section....
%%Equation .... is the final form of the equalizer.
%%This equation is repeated here
%%\begin{equation}
%%\text{FDE}_1 = \frac{\mathbf{H}^*}{\|\mathbf{H}\|^2 + \sigma_0^2}
%%\label{eq:FDE1}
%%\end{equation}
%%Equation \eqref{eq:FDE1} is extremely easy to implement in the GPU.
%%All that is need to calculate the equalizer is the Fourier Transform of $\hat{\mathbf{h}}$ and the noise variance $\sigma_0^2$.
%%As shown in Figure \ref{fig:FDE1_block}, the GPU zero pads that channel estimate then takes the $N_{FFT}$ point FFT to obtain $\mathbf{H}$.
%%Equation \eqref{eq:FDE1} is then calculated by launching $N_{FFT}$ threads per packet.
%%The output of the multiply $\text{FDE}_1$ is the Frequency Domain Equalizer left in the  frequency domain.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*5]{figures/gpu/FDE1_block.png}
%%	\caption{The block diagram showing how the GPU calculates the $\text{FDE}_1$ equalizer.}
%%	\label{fig:FDE1_block}
%%\end{figure}
%%
%%\subsubsection{Frequency Domain Equalizer 2}
%%The Frequency Domain Equalizer 2 (FDE2) was studied in section....
%%Equation .... is the final form of the equalizer.
%%$\text{FDE}_2$ is very simmilar to $\text{FDE}_1$, $\text{FDE}_2$ has an extra term on $\sigma_0^2$
%%This equation is repeated here
%%\begin{equation}
%%\text{FDE}_2 = \frac{\mathbf{H}^*}{\|\mathbf{H}\|^2 + \sigma_0^2 \mathbf{\Psi}}
%%\label{eq:FDE2}
%%\end{equation}
%%Equation \eqref{eq:FDE2} is implemented the same way \eqref{eq:FDE1} but $\text{FDE}_2$ has one extra vector $\mathbf{\Psi}$ in the multiply.
%%The vector $\mathbf{\Psi}$ is recomputed and stored.
%%The output of the multiply $\text{FDE}_2$ is the Frequency Domain Equalizer left in the  frequency domain.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*5]{figures/gpu/FDE2_block.png}
%%	\caption{The block diagram showing how the GPU calculates the $\text{FDE}_2$ equalizer.}
%%	\label{fig:FDE2_block}
%%\end{figure}
%%
%%\subsection{Filter Application GPU Implementation}
%%With all the equalizers calculated and ready to apply, the GPU applys the FIR equalizers and the Numerically Optimized detection filter \cite{NO_perrins}.
%%The Perrins detection filter length $L_{df}$ is 21 samples long with the center tap at index 10.
%%
%%\subsubsection{FIR Equalizer Application}
%%The FIR equalizers (ZF, MMSE and CMA) and the Perrins detection filter are all applied using the convolution block shown in Figure \ref{fig:conv_FFT_block3} and \ref{fig:conv_simple_block}.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*5]{figures/gpu/apply_FIR_equalizer.png}
%%	\caption{The block diagram showing how the GPU applies an FIR equlizer and the Numerically Optimized Perrins detection filter.}
%%	\label{fig:apply_FIR_equalizer}
%%\end{figure}
%%The derotated samples vector $\mathbf{r}_r$ is $L_{pkt}$ or $12672$ samples long.
%%The calculated equalizer $\mathbf{c}$ is $L_{eq}$ or $186$ samples long.
%%The Perrins Numerically Optimized detection filter $\mathbf{d}$ is $L_{df}$ or $21$ samples long.
%%The $L_{pkt}$ samples that need to be pruned out start at $L_1+L_{df}/2$.
%%The vector $\mathbf{y}$ are samples that have been equalized and detected, ready for the symbol detector and Phase Lock Loop.
%%
%%\clearpage
%%\subsubsection{Frequency Domain Equalizer Application}
%%The application of the Frequency Domain Equalizers, $\text{FDE}_1$ and $\text{FDE}_2$, are not applied using the conventional convolution.
%%$\text{FDE}_1$ and $\text{FDE}_2$ are applied in the frequency domain by
%%\begin{equation}
%%\text{FDE}_2 = \frac{\mathbf{H}^*\mathbf{R}_r \mathbf{D}}{\|\mathbf{H}\|^2 + \sigma_0^2}
%%\label{eq:apply_FDE1}
%%\end{equation}
%%and
%%\begin{equation}
%%\text{FDE}_2 = \frac{\mathbf{H}^*\mathbf{R}_r \mathbf{D}}{\|\mathbf{H}\|^2 + \sigma_0^2 \mathbf{\Psi}}
%%\label{eq:apply_FDE2}
%%\end{equation}
%%where $\mathbf{R}_r$ is the Fourier transform of the zero padded derotated samples $\mathbf{r}_r$ and $\mathbf{D}$ is the Fourier transform of the zero padded detection filter $\mathbf{d}$.
%%The Fourier transform of Perrins Detection filter was initialized and stored.
%%
%%Figures \ref{fig:apply_FDE1} and \ref{fig:apply_FDE2} show block diagrams of how $\text{FDE}_1$ and $\text{FDE}_2$ is implemented in the GPU.
%%The vector $\mathbf{\Psi}$ was precomputed and stored at initialization.
%%The multiply in each figure is not a typical multiplication, the multiply block implements Equations \eqref{eq:FDE1} and \eqref{eq:FDE2}.
%%To prepare the data for the OQPSK demodulator and Phase Lock Loop, the prune block shown in both figures down samples by 2.
%%With the signal downsampled to 1 sample per bit, the equalized received signal is ready for the demodulator, Phase Lock Loop and bit decisions.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/apply_FDE1.png}
%%	\caption{The block diagram showing how the GPU applies $\text{FDE}_1$ and the Numerically Optimized Perrins detection filter.}
%%	\label{fig:apply_FDE1}
%%\end{figure}
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/apply_FDE2.png}
%%	\caption{The block diagram showing how the GPU applies $\text{FDE}_2$ with $\mathbf{\Psi}$ and the Numerically Optimized Perrins detection filter.}
%%	\label{fig:apply_FDE2}
%%\end{figure}
%%
%%\clearpage
%%\subsection{GPU OQPSK Demodulator Implementation}
%%After all the equalizers and detection filters have been applied to the received samples there is 5 different streams of equalized samples.
%%The bit decisions from each stream of equalized samples needs to be obtained.
%%
%%The top of Figure \ref{fig:demod_block} shows a block diagram of how the bit decisions are made.
%%A "symbol-by-symbol" OQPSK detector is used to build the data decision vector $\hat{\mathbf{a}}$ for each stream of equalized samples.
%%The OQPSK detector is first data-aided then decision directed. 
%%In case the frequency offset estimator in section \ref{sec:jeffs_frequencyoffsetestimator} was imperfect, a first order  with a Phase Lock Loop (PLL) is applied to track out the residual frequency offset.
%%Figure \ref{fig:demod_loop} shows a block diagram of a first order PLL.
%%
%%Traditionally a single PLL would be applied to the whole stream of equalized samples because PLLs are inherently serial, but the equalized samples have a packet structure with known data.
%%A PLL can be applied to each packet introducing parallelism to map best to GPUs.
%%One thread per packet per stream of equalized samples is launched to run the OQPSK demodulator shown in Figure \ref{fig:demod_loop}.
%%
%%Starting at the in the preamble of each packet, the PLL tracks out the a frequency offset for $0\leq k \leq L_{pkt}$ by estimating the maximum likelihood phase error $e(k)$ where
%%\begin{equation*}
%%e(k)
%%= 
%%\begin{cases}
%%0 &\text{$k$ even} \\
%%\hat{a}(k-1)\mathbb{I}\{y_r(k-1)\} -  \hat{a}(k)\mathbb{R}\{y_r(k)\}  &\text{$k$ odd}\\
%%\end{cases}.
%%\end{equation*}
%%With the estimated phase error estimated, the gain $K_1$ is applied to $e(k)$ and a Direct Digital Synthesizer (DDS) generates a signal to derotate the equalized sample at index $k$.
%%For every $y(k)$ in the packet, a derotated sample $y_r(k)$ is calculated and the data decision $\hat{a}(k)$ is estimated where
%%\begin{equation}
%%\hat{a}(k)= \begin{cases}
%%p(k) &k<L_p+L_{asm} \\
%%sgn(\mathbb{R}\{y_{r}(k)\})&k\geq L_p+L_{asm} \quad \& \quad \text{$k$ even}\\
%%sgn(\mathbb{I}\{y_{r}(k)\})&k\geq L_p+L_{asm} \quad \& \quad \text{$k$ odd}\\
%%\end{cases}.
%%\end{equation}
%%and $p(k)$ are the known bits or decisions in the preamble and ASM.
%%Once the index $k$ is out of the preamble and asm, the data decisions $\hat{a}(k)$ are based on the sign of the real $\mathbb{R}$ or imaginary $\mathbb{I}$part of $y_r(k)$. 
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*5]{figures/gpu/demod_block.png}
%%	\caption{The block diagram showing how the GPU applies the demodulator to equalized received samples to obtain the bit decisions.}
%%	\label{fig:demod_block}
%%\end{figure}
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/demod_loop.png}
%%	\caption{The block diagram showing the OQPSK demodulator from Figure \ref{fig:demod_block}. The $\hat{a}(k)$ is data-aided for $k<L_p+L_{asm}$ and decision directed when $k\geq L_p+L_{asm}$.}
%%	\label{fig:demod_loop}
%%\end{figure}
%%
%%After the vector $\hat{\mathbf{a}}$ is calculated, the data bits are pruned out of every packet as shown in Figure \ref{fig:demod_block} as the bit prune block.
%%One thread per data bit is launched to strip the data bits and build the bit decision vector $\hat{\mathbf{b}}$ from $\hat{\mathbf{a}}$ where
%%\begin{equation}
%%\hat{b}(n)
%%= 
%%\begin{cases}
%%0 &\hat{a}(n)>0 \\
%%1 &\hat{a}(n)<0\\
%%\end{cases}.
%%\end{equation}
%%After the data bit vector $\hat{\mathbf{b}}$ for each equalized stream is built, the bit decisions are all transferred to the Tesla k40 GPU in the copy bits block.
%%The bit vector $\hat{\mathbf{s}}_{FPGA}$ is a c++ char array containing interleaved bit streams $\hat{\mathbf{b}}$ from each stream of equalized samples. 
%%In each char index of $\hat{\mathbf{s}}_{FPGA}$ contains 8 bit decisions for a single equalized bit stream.
%%The first bit stream in $\hat{\mathbf{s}}_{FPGA}$ is the bit stream from the ZF equalizer, followed by MMSE, CMA, $\text{FDE}_1$, $\text{FDE}_2$ then 3 blank streams.
%%Figure \ref{fig:bit_inter} shows how the pattern repeats every 8 indices in the char array $\hat{\mathbf{s}}_{FPGA}$.
%%This array $\hat{\mathbf{s}}_{FPGA}$ containing bit decisions from each equalizer is burst into the FPGA then clocked out to the Bit Error Rate Tester.
%%\begin{figure}
%%	\centering\includegraphics[width=\textwidth/10*8]{figures/gpu/bit_inter.png}
%%	\caption{The bit stream vector $\hat{\mathbf{s}}_{FPGA}$ is array chars. The bit streams are interleaved and each char contains 8 bit decisions from a single bit stream.}
%%	\label{fig:bit_inter}
%%\end{figure}
% 