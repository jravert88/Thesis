\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces A block diagram of the estimation process.}}{5}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces A block diagram of the equalization and symbol detector process.}}{6}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces The iNET packet structure.}}{6}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Offset Quadriture Phase Shift Keying symbol by symbol detector.}}{10}{figure.3.4}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces A block diagram of how a CPU sequentially performs vector addition.}}{12}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces A block diagram of how a GPU performs vector addition in parallel.}}{12}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Block $0$ $32$ threads launched in $4$ thread blocks with $8$ threads per block.}}{15}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces $36$ threads launched in $5$ thread blocks with $8$ threads per block with $4$ idle threads.}}{15}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces A block diagram where local, shared, and global memory is located. Each thread has private local memory. Each thread block has private shared memory. The GPU has global memory that all threads can access.}}{16}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces NVIDIA Tesla K40c and K20c.}}{17}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Example of an NVIDIA GPU card. The SRAM is shown to be boxed in yellow. The GPU chip is shown to be boxed in red.}}{17}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces The GPU convolution thread optimization of a $12672$ length signal with a $186$ tap filter using shared memory. $192$ is the optimal number of threads per block executing in $0.1101$ms. Note that at least $186$ threads per block must be launched to compute correct output.}}{20}{figure.4.8}
\contentsline {figure}{\numberline {4.9}{\ignorespaces ConvGPU thread optimization 128 threads per block 0.006811.}}{21}{figure.4.9}
\contentsline {figure}{\numberline {4.10}{\ignorespaces The typical approach of CPU and GPU operations. This block diagram shows a Profile of Listing \ref {code:noPipe}.}}{21}{figure.4.10}
\contentsline {figure}{\numberline {4.11}{\ignorespaces GPU and CPU operations can be pipelined. This block diagram shows a Profile of Listing \ref {code:pipe}.}}{22}{figure.4.11}
\contentsline {figure}{\numberline {4.12}{\ignorespaces A block diagram of pipelining a CPU with three GPUs.}}{23}{figure.4.12}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Comparison of number of floating point operations (flops) required to convolve a $12672$ sample complex signal with a $186$ tap complex filter.}}{27}{figure.5.1}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison of number of floating point operations (flops) required to convolve a $12672$ sample complex signal with a $21$ tap complex filter.}}{28}{figure.5.2}
\contentsline {figure}{\numberline {5.3}{\ignorespaces Comparison of a complex convolution on CPU verse GPU. The signal length is varied and the filter is fixed at $186$ taps. The comparison is messy with out lower bounding.}}{30}{figure.5.3}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Comparison of a complex convolution on CPU verse GPU. The signal length is varied and the filter is fixed at $186$ taps. A lower bound was applied by searching for a local minimums in $15$ sample width windows.}}{31}{figure.5.4}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Comparison of a complex convolution on CPU verse GPU. The signal length is varied and the filter is fixed at $21$ taps. A lower bound was applied by searching for a local minimums in $5$ sample width windows.}}{32}{figure.5.5}
\contentsline {figure}{\numberline {5.6}{\ignorespaces Comparison of a complex convolution on CPU verse GPU. The filter length is varied and the signal is fixed at $12672$ samples. A lower bound was applied by searching for a local minimums in $3$ sample width windows.}}{33}{figure.5.6}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Comparison of a batched complex convolution on a CPU and GPU. The number of batches is varied while the signal and filter length is set to $12672$ and $186$.}}{34}{figure.5.7}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Comparison of a batched complex convolution on a GPU. The signal length is varied and the filter is fixed at $186$ taps.}}{36}{figure.5.8}
\contentsline {figure}{\numberline {5.9}{\ignorespaces Comparison of a batched complex convolution on a GPU. The signal length is varied and the filter is fixed at $21$ taps.}}{37}{figure.5.9}
\contentsline {figure}{\numberline {5.10}{\ignorespaces Comparison of a batched complex convolution on a GPU. The signal length is varied and the filter is fixed at $21$ taps.}}{38}{figure.5.10}
\contentsline {figure}{\numberline {5.11}{\ignorespaces Two ways to convolve the signal $\mathbf {r}$ with the $186$ tap filter $\mathbf {c}$ and $21$ tap filter $\mathbf {d}$.}}{39}{figure.5.11}
\contentsline {figure}{\numberline {5.12}{\ignorespaces Comparison of number of floating point operations (flops) required to convolve a $12672$ sample complex signal with a $186$ tap complex filter.}}{41}{figure.5.12}
\contentsline {figure}{\numberline {5.13}{\ignorespaces Comparison of number of floating point operations (flops) required to convolve a $12672$ sample complex signal with a $21$ tap complex filter.}}{42}{figure.5.13}
\contentsline {figure}{\numberline {5.14}{\ignorespaces Comparison of a complex convolution on CPU verse GPU. The signal length is varied and the filter is fixed at $186$ taps. The comparison is messy with out lower bounding.}}{44}{figure.5.14}
\contentsline {figure}{\numberline {5.15}{\ignorespaces Comparison of a complex convolution on CPU verse GPU. The signal length is varied and the filter is fixed at $186$ taps. A lower bound was applied by searching for a local minimums in $15$ sample width windows.}}{45}{figure.5.15}
\contentsline {figure}{\numberline {5.16}{\ignorespaces Comparison of a complex convolution on CPU verse GPU. The signal length is varied and the filter is fixed at $21$ taps. A lower bound was applied by searching for a local minimums in $5$ sample width windows.}}{46}{figure.5.16}
\contentsline {figure}{\numberline {5.17}{\ignorespaces Comparison of a complex convolution on CPU verse GPU. The filter length is varied and the signal is fixed at $12672$ samples. A lower bound was applied by searching for a local minimums in $3$ sample width windows.}}{47}{figure.5.17}
\contentsline {figure}{\numberline {5.18}{\ignorespaces Comparison of a batched complex convolution on a CPU and GPU. The number of batches is varied while the signal and filter length is set to $12672$ and $186$.}}{49}{figure.5.18}
\contentsline {figure}{\numberline {5.19}{\ignorespaces Comparison of a batched complex convolution on a GPU. The signal length is varied and the filter is fixed at $186$ taps.}}{50}{figure.5.19}
\contentsline {figure}{\numberline {5.20}{\ignorespaces Comparison of a batched complex convolution on a GPU. The signal length is varied and the filter is fixed at $21$ taps.}}{51}{figure.5.20}
\contentsline {figure}{\numberline {5.21}{\ignorespaces Comparison of a batched complex convolution on a GPU. The signal length is varied and the filter is fixed at $21$ taps.}}{52}{figure.5.21}
\contentsline {figure}{\numberline {5.22}{\ignorespaces Two ways to convolve the signal $\mathbf {r}$ with the $186$ tap filter $\mathbf {c}$ and $21$ tap filter $\mathbf {d}$.}}{54}{figure.5.22}
\contentsline {figure}{\numberline {5.23}{\ignorespaces Comparison of a batched cascaded complex convolution on a GPU. The signal length is varied and the filter is the $206$ result of convolving $186$ and $21$ tap filters.}}{55}{figure.5.23}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Diagram showing the relationships between $z(n)$, $\rho (n)$ and $b(n)$.}}{75}{figure.6.1}
\contentsline {figure}{\numberline {6.2}{\ignorespaces I need help on this one!!!! }}{77}{figure.6.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {7.1}{\ignorespaces Diagram showing the relationships between $z(n)$, $\rho (n)$ and $b(n)$.}}{80}{figure.7.1}
\contentsline {figure}{\numberline {7.2}{\ignorespaces Diagram showing the relationships between $z(n)$, $\rho (n)$ and $b(n)$.}}{80}{figure.7.2}
\contentsline {figure}{\numberline {7.3}{\ignorespaces Diagram showing the relationships between $z(n)$, $\rho (n)$ and $b(n)$.}}{83}{figure.7.3}
\contentsline {figure}{\numberline {7.4}{\ignorespaces Diagram showing the relationships between $z(n)$, $\rho (n)$ and $b(n)$.}}{84}{figure.7.4}
\contentsline {figure}{\numberline {7.5}{\ignorespaces Diagram showing the relationships between $z(n)$, $\rho (n)$ and $b(n)$.}}{85}{figure.7.5}
\contentsline {figure}{\numberline {7.6}{\ignorespaces Diagram showing the relationships between $z(n)$, $\rho (n)$ and $b(n)$.}}{86}{figure.7.6}
\contentsline {figure}{\numberline {7.7}{\ignorespaces Diagram showing the relationships between $z(n)$, $\rho (n)$ and $b(n)$.}}{86}{figure.7.7}
\addvspace {10\p@ }
\addvspace {10\p@ }
