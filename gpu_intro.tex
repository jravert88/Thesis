%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%% GPU
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% \cleardoublepage
\chapter{Signal Processing with GPUs}
\label{sec:gpu}

This thesis explores the use of GPUs in data-aided estimation, equalization and filtering operations.

A Graphics Processing Unit (GPU) is a computational unit with a highly-parallel architecture well-suited for executing the same function on many data elements.
In the past, GPUs were used to process graphics data.
Recently, general purpose GPUs are being used for high performance computing in computer vision, deep learning, artificial intelligence and signal processing \cite{wikipedia-gpu:2015}.

GPUs cannot be programmed the way as a CPU. 
NVIDIA released a extension to C, C++ and Fortran called CUDA (Compute Unified Device Architecture).
CUDA allows a programmer to write C++ like functions that are massively parallel called \textit{kernels}.
To invoke parallelism, a GPU kernel is called $N$ times and mapped to $N$ \textit{threads} that run concurrently.
To achieve the full potential of high performance GPUs, kernels must be written with some basic concepts about GPU architecture and memory in mind.

The purpose of this overview is to provide context for the contributions of this thesis.
As such this overview is not a tutorial.
For a full explination of CUDA programming please see the CUDA toolkit documentation \cite{CUDA_toolkit_doc}.

\section{Simple GPU code example}
If a programmer has some C++ experience, learning how to program GPUs using CUDA comes fairly easily.
GPU code still runs top to bottom and memory still has to be allocated.
The only real difference is where the memory physically is and how functions run on GPUs.
To run functions or kernels on GPUs, the memory must be copied from the host (CPU) to the device (GPU).
Once the memory has been copied, the parallel GPU kernels can be called.
After the GPU kernels have finished, the resulting memory has to be copied back from the device (GPU) to the host (CPU).

Listing \ref{code:GPUvsCPU} shows a simple program that sums to vectors together
\begin{equation}
\begin{matrix}
\mathbf{C}_1 = \mathbf{A}_1 + \mathbf{B}_1 \\
\mathbf{C}_2 = \mathbf{A}_2 + \mathbf{B}_2
\end{matrix}
\end{equation}
where each vector is length $1024$.
Figure \ref{fig:CPUaddBlockDiagram} shows how the CPU computes $\mathbf{C}_1$ by summing elements of $\mathbf{A}_1$ and $\mathbf{B}_1$ together \textit{sequentially}.
Figure \ref{fig:GPUaddBlockDiagram} shows how the GPU computes $\mathbf{C}_2$ by summing elements of $\mathbf{A}_1$ and $\mathbf{B}_1$ together \textit{in parallel}.
The GPU kernel computes every element of $\mathbf{C}_2$ in parallel while the CPU computes one element of $\mathbf{C}_1$ at a time.
\begin{figure}
	\centering\includegraphics[width=3.17in/100*55]{figures/gpu_intro/CPUaddBlockDiagram.pdf}
	\caption{A block diagram of how a CPU sequentially performs vector addition.}
	\label{fig:CPUaddBlockDiagram}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=4.69in/100*55]{figures/gpu_intro/GPUaddBlockDiagram.pdf}
	\caption{A block diagram of how a GPU performs vector addition in parallel.}
	\label{fig:GPUaddBlockDiagram}
\end{figure}
\singlespacing
\clearpage
\begin{lstlisting}[caption={Comparison of CPU verse GPU code.},label={code:GPUvsCPU}]
#include <iostream>
#include <stdlib.h>
#include <math.h>
using namespace std;

void VecAddCPU(float* destination,float* source0,float* source1,int myLength){
	for(int i = 0; i < myLength; i++)
		destination[i] = source0[i] + source1[i];
}

__global__ void VecAddGPU(float* destination, float* source0, float* source1, int lastThread){
	int i = blockIdx.x*blockDim.x + threadIdx.x;

	// don't access elements out of bounds
	if(i >= lastThread)
		return;

	destination[i] = source0[i] + source1[i];
}


int main(){
	int numPoints = pow(2,22);
	cout << numPoints << endl;
	/*--------------------------------------------------------------------
                               	   CPU Start
	--------------------------------------------------------------------*/
	// allocate memory on host
	float *A1;
	float *B1;
	float *C1;
	A1 = (float*) malloc (numPoints*sizeof(float));
	B1 = (float*) malloc (numPoints*sizeof(float));
	C1 = (float*) malloc (numPoints*sizeof(float));

	// Initialize vectors 0-99
	for(int i = 0; i < numPoints; i++){
		A1[i] = rand()%100;
		B1[i] = rand()%100;
	}

	// vector sum C1 = A1 + B1
	VecAddCPU(C1, A1, B1, numPoints);
	/*--------------------------------------------------------------------
                               	   CPU End
	--------------------------------------------------------------------*/

	/*--------------------------------------------------------------------
                               	   GPU End
	--------------------------------------------------------------------*/
	// allocate memory on host for result
	float *C2;
	C2 = (float*) malloc (numPoints*sizeof(float));

	// allocate memory on device for computation
	float *A2_gpu;
	float *B2_gpu;
	float *C2_gpu;
	cudaMalloc(&A2_gpu, sizeof(float)*numPoints);
	cudaMalloc(&B2_gpu, sizeof(float)*numPoints);
	cudaMalloc(&C2_gpu, sizeof(float)*numPoints);

	// Copy vectors A and B from host to device
	cudaMemcpy(A2_gpu, A1, sizeof(float)*numPoints, cudaMemcpyHostToDevice);
	cudaMemcpy(B2_gpu, B1, sizeof(float)*numPoints, cudaMemcpyHostToDevice);

	// Set optimal number of threads per block
	int numTreadsPerBlock = 32;

	// Compute number of blocks for set number of threads
	int numBlocks = numPoints/numTreadsPerBlock;

	// If there are left over points, run an extra block
	if(numPoints % numTreadsPerBlock > 0)
		numBlocks++;

	// Run computation on device
	//for(int i = 0; i < 100; i++)
	VecAddGPU<<<numBlocks, numTreadsPerBlock>>>(C2_gpu, A2_gpu, B2_gpu, numPoints);

	// Copy vector C2 from device to host
	cudaMemcpy(C2, C2_gpu, sizeof(float)*numPoints, cudaMemcpyDeviceToHost);
	/*--------------------------------------------------------------------
                               	   GPU End
	--------------------------------------------------------------------*/

	// Compare C2 to C1
	bool equal = true;
	for(int i = 0; i < numPoints; i++)
		if(C1[i] != C2[i])
			equal = false;
	if(equal)
		cout << "C2 is equal to C1." << endl;
	else
		cout << "C2 is NOT equal to C1." << endl;
	sleep(2);

	// Free vectors on CPU
	free(A1);
	free(B1);
	free(C1);
	free(C2);

	// Free vectors on GPU
	cudaFree(A2_gpu);
	cudaFree(B2_gpu);
	cudaFree(C2_gpu);
}
\end{lstlisting}
\doublespacing

\section{GPU kernel using threads and thread blocks}
A GPU kernel is executed on a GPU by launching numTreadsPerBlock$\times$numBlocks 
threads.
Each thread has a unique index.
CUDA calls this indes threadIdx and blockIdx.
threadIdx is the thread index inside the assigned thread block.
blockIdx is the index of the block the thread is assigned.
blockDim is the number of threads assigned per block, in fact blockDim $=$ numTreadsPerBlock.
Both threadIdx and blockIdx are three dimensional and have x, y and z components.
In this thesis only the x dimension is used because GPU kernels operate only on vectors.

To replace a CPU for loop that runs $0$ to $N-1$, a GPU kernel launches $N$ threads are with $T$ threads per thread block.
The number of blocks need is $M = \frac{N}{T}$ or $M = \frac{N}{T}+1$ if $N$ is not an integer multiple of $T$.
Figure \ref{fig:threadsBlocks32} shows $32$ threads launched in $4$ thread blocks with $8$ threads per block.
Figure \ref{fig:threadsBlocks36} shows $36$ threads launched in $5$ thread blocks with $8$ threads per block. An full extra thread block must be launched to with $8$ threads but $4$ threads are idle.
\begin{figure}
	\centering\includegraphics[width=4in/100*55]{figures/gpu_intro/threadsBlocks32.pdf}
	\caption{Block $0$ $32$ threads launched in $4$ thread blocks with $8$ threads per block.}
	\label{fig:threadsBlocks32}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=5in/100*55]{figures/gpu_intro/threadsBlocks36.pdf}
	\caption{$36$ threads launched in $5$ thread blocks with $8$ threads per block with $4$ idle threads.}
	\label{fig:threadsBlocks36}
\end{figure}

\section{GPU memory}
Thread blocks run independent of other thread blocks.
The GPU does not guarantee Block $0$ will execute before Block $2$.
Threads in blocks can coordinate and use shared memory but blocks do not coordinate with other blocks.
Threads have access to private local memory that is fast and efficient.
Each thread in a thread block has access to private shared memory in the thread block.
All threads have access to global memory.

Local memory is the fastest and global memory is by far the slowest.
One global memory access takes 400-800 clock cycles while a local memory is a few clock cycles.
Why not just do all computations in local memory?
The memory needs come from global memory to before it can be used in local memory.
Memory should be saved in shared memory if many threads are going to use it in a thread block.
Local and shared memory should be used as much as possible but sometimes a GPU kernel cant utilized local and shared memory because elements might only be used once.
\begin{sidewaysfigure}
	\centering\includegraphics[width=9in/100*55]{figures/gpu_intro/fullGPUmemBlockDiagram.png}
	\caption{A block diagram where local, shared, and global memory is located. Each thread has private local memory. Each thread block has private shared memory. The GPU has global memory that all threads can access.}
	\label{fig:fullGPUmemBlockDiagram}
\end{sidewaysfigure}

Why is global memory so slow?
Looking at the physical hardware will shed some light.
This thesis uses NVIDIA Tesla K40c and K20c GPUs, Table \ref{tab:gpu-resources_jeffs} gives some specifications and Figure \ref{fig:GPUpicture} shows the form factor of the these GPUs.
The red box in Figure \ref{fig:GPUarch} show the GPU chip and the yellow boxes show the SRAM that is \textit{off} the GPU chip.
The GPU global memory is located in the SRAM.
To move memory to thread blocks \textit{on} the GPU chip from global memory requires fetching memory from \textit{off} the GPU.
Now 400-800 clock cycles doesn't sound all the bad huh?
\begin{figure}
	\centering\includegraphics[width=5in]{figures/gpu_intro/k40c_k20c.jpg}
	\caption{NVIDIA Tesla K40c and K20c.}
	\label{fig:GPUpicture}
\end{figure}
\begin{figure}
	\centering\includegraphics[width=\textwidth]{figures/gpu_intro/Kepler_box.png}
	\caption{Example of an NVIDIA GPU card. The SRAM is shown to be boxed in yellow. The GPU chip is shown to be boxed in red.}
	\label{fig:GPUarch}
\end{figure}
\begin{table}
\begin{center}
\begin{tabular}{lll}
	\toprule
	Feature 			& Tesla K40c 	& Tesla K20c 	\\ \midrule
	Memory size (GDDR5) & 12 GB 		& 5 GB 			\\
	CUDA cores 			& 2880 			& 2496 			\\
	Base clock (MHz) 	& 745 			& 732 			\\ \bottomrule
\end{tabular}
\end{center}
\caption{The computational resources available with three NVIDIA GPUs used in this thesis (1x Tesla K40c 2x Tesla K20c).}
\label{tab:gpu-resources_jeffs}
\end{table}

\section{Cuda Libraries}
CUDA isn't just a programming language, it also has many libraries that are extreemly useful that are optimized for NVIDIA GPUs.
CUDA libraries are written by NVIDIA engineers that know how to squeeze out every drop of performance out of NVIDIA GPUs.
Because ninjas are unbeatable, NVIDIA engineers are known as ninjas in the Telemetry Group at BYU.
While figuring out how to optimize a GPU kernel is extremely satisfying, GPU programmers should always search the CUDA libraries for any thing that might be useful.

Some libraries used in this Thesis are
\begin{itemize}
  \item cufft
  \item cublas
  \item cusolver
  \item cusolverSp
\end{itemize}

\section{Cuda Convolution}
An important tool to Digital Communications and Digital Signal Processing is convolution.
Time domain convolution is
\begin{align}
y(n) = \sum^{L_\text{h}-1}_{m=0} x(m) h(n-m)
	 \label{eq:simple_conv}
\end{align}
where $x(m)$, $h(n-m)$ and $y(n)$ are complex. 
While Listing \ref{code:GPUvsCPU} is a simple and good example showing how program GPUs, frankly, it is pretty boring and doesn't display the real challenges and tradeoffs of GPUs.
Listing \ref{code:convFun} shows four different ways of implementing convolution
\begin{itemize}
  \item time domain convolution in a CPU
  \item time domain convolution in a GPU using global memory
  \item time domain convolution in a GPU using shared memory
  \item frequency domain convolution in a GPU using CUDA libraries
\end{itemize}
The CPU implements Equation \eqref{eq:simple_conv} in ConvCPU directly from line $6$ to $30$.
The GPU implements time domain convolution using global memory in the GPU kernel ConvGPU.
ConvGPU on lines $32$ to $59$ is a parallel of ConvCPU.
ConvGPU implements time domain convolution by accessing global memory for any needed element of the signal and filter.

Threads accessing the same elements of the filter in global memory seems like a waste of valuable clock cycles.
The GPU kernel ConvGPUshared on lines $61$ to $96$ differs slightly from ConvGPU by making $L_\text{h}$ threads save the filter to shared memory from global memory.

Any graduate of a signal processing class knows the trade off of convolution in the time verse frequency domain.
Time domain convolution takes $\approx L_\text{s} L_\text{h}$ complex multiplies where $L_\text{s}$ is the signal length and $L_\text{h}$ is the filter length.
Frequency domain convolution takes $\approx \frac{3 N_\text{FFT}}{2} \log_2(N_\text{FFT}) + N_\text{FFT}$ complex multiplies where $N_\text{FFT}$ is the next multiple of $2$ above the convolution length $L_\text{s}+L_\text{h}-1$.

Lines $213$ to $232$ show how to do frequency domain convolution using the cuFFT library and two simple GPU kernels.

So the questions are:
When should I stay on the host and run my convolution on the CPU?
When should I copy my vectors to the GPU, run my convolution, then copy the vectors back to the host?
If I am going to run my convolution the GPU, when should I only use global memory?
When should I use shared memory?
When should I do convolution in the frequency domain?

The answer to all of the questions is...it depends on your signal length, filter length, CPU, GPU and memory.

A CUDA programmer can make an educated guess on which way may be faster, but until the algorithms have been implemented and timed, there is no definite answer.
Figures \ref{fig:CPUvsGPU_spikes} to \ref{taps10CPUvsGPU} compare the four different ways of doing convolution implemented in Listing \ref{code:GPUvsCPU} on a Tesla K40c GPU.
The filter length for Figures \ref{fig:CPUvsGPU_spikes} to \ref{CPUvsGPU_GPUtoGPU} is $186$.
The filter length for Figure \ref{fig:taps10CPUvsGPU} is $10$.
Table \ref{tab:CPUvsGPUtimingTable} lists how the kernels were timed.
Note that all memory transfers from host to device and device to host were included in execution times.
\begin{table}
\caption{Defining start and stop lines for timing comparison in Listing \ref{code:convFun}.}
\begin{center}
\begin{tabular}{lll}
	\toprule
	Algorithm 								& Start Timing Line	& Stop Timing Line	\\ \midrule
	CPU time domain (ConvCPU) 				& 182				& 183 				\\
	GPU time domain global (ConvGPU) 		& 187				& 195				\\
	GPU time domain shared (ConvGPUshared) 	& 200				& 208				\\
	GPU frequency domain 					& 212				& 231				\\ 
	\bottomrule
\end{tabular}
\end{center}
\label{tab:CPUvsGPUtimingTable}
\end{table}

Figure \ref{fig:CPUvsGPU_spikes} shows the raw computation time on the K40c for each algorithm in Table \ref{tab:CPUvsGPUtimingTable} with the filter length set to $186$.
The figure has many irregularities because the operating system doesn't always provide as much resources as needed.

Figure \ref{fig:CPUvsGPU} is generated from the raw computation data but lower bounded by searching for a minimum every $20$ samples.
Judging my the Figure, for long signals convolved with $186$ tap filters, GPU convolution is much faster than CPU.
\begin{figure}
	\caption{Comparison of complex convolution on CPU to GPU with varying signal lengths without lower bounding.}
	\centering\includegraphics[width=5in]{figures/gpu_intro/CPUvsGPU_spikes.eps}
	\label{fig:CPUvsGPU_spikes}
\end{figure}
\begin{figure}
	\caption{Comparison of complex convolution on CPU to GPU with varying signal lengths with lower bounding.}
	\centering\includegraphics[width=5in]{figures/gpu_intro/CPUvsGPU.eps}
	\label{fig:CPUvsGPU}
\end{figure}

Looking at the bottom left of Figures \ref{fig:CPUvsGPU_spikes} and \ref{fig:CPUvsGPU}, a DSP engineer would ask, ``Is it ever better to do convolution on the CPU rather than GPU?''
Yes, but for very short signals with a $186$ tap filter.
Figure \ref{fig:CPUvsGPU_CPUtoGPU} shows when a CPU is faster than a GPU.
\begin{figure}
	\caption{Plot showing when CPU convolution is faster than GPU convolution.}
	\centering\includegraphics[width=5in]{figures/gpu_intro/CPUvsGPU_CPUtoGPU.eps}
	\label{fig:CPUvsGPU_CPUtoGPU}
\end{figure}

Now for the interesting question, if I am going to do convolution in the GPU, should I do time domain or frequency domain convolution?
If I do time domain convolution, should I use global or shared memory?
Figure \ref{fig:CPUvsGPU_GPUtoGPU} shows that the answer: ``It depends.''
A good choice is to do frequency domain, but not always.
As the signal length increases, the frequency domain execution time has a staircase shape because the signal is zero padded out to the next power of 2 to leverage the Cooley-Tukey FFT algorithm \cite{Cooley-Tukey_old}.
\begin{figure}
	\caption{Plot showing trade offs with convolution in GPUs.}
	\centering\includegraphics[width=5in]{figures/gpu_intro/CPUvsGPU_GPUtoGPU.eps}
	\label{fig:CPUvsGPU_GPUtoGPU}
\end{figure}

Usually, when implementing convolution, the signal and filter length is set.
If is good practice to implement convolution in the CPU and GPU in many ways to evaluate which way is best.
Choosing the signal length to be $2^{15}$ and the filter length to be $186$, Tables \ref{tab:CPUvsGPUtable_2_15} and \ref{tab:CPUvsGPUtable_12672} shows how the different algorithms compare for different signal lengths.
\begin{table}
\caption{Convolution computation times with signal length $2^{15} = 32768$ and filter length $186$ on a Tesla K40c GPU.}
\begin{center}
\begin{tabular}{lll}
	\toprule
	Algorithm 								& time (ms) \\ \midrule
	CPU time domain (ConvCPU) 				& 131.943 	\\
	GPU time domain global (ConvGPU) 		& 7.79477	\\
	GPU time domain shared (ConvGPUshared) 	& 6.77736	\\
	GPU frequency domain		 			& 5.81085	\\ 
	\bottomrule
\end{tabular}
\end{center}
\label{tab:CPUvsGPUtable_2_15}
\end{table}
\begin{table}
\caption{Convolution computation times with signal length $12672$ and filter length $186$ on a Tesla K40c GPU.}
\begin{center}
\begin{tabular}{lll}
	\toprule
	Algorithm 								& time (ms) \\ \midrule
	CPU time domain (ConvCPU) 				& 5.04636 	\\
	GPU time domain global (ConvGPU) 		& 0.320013	\\
	GPU time domain shared (ConvGPUshared) 	& 0.292616	\\
	GPU frequency domain		 			& 0.284187	\\ 
	\bottomrule
\end{tabular}
\end{center}
\label{tab:CPUvsGPUtable_12672}
\end{table}

If the length of the filter were changed from $186$ to $10$ taps, everything changes.
Figure \ref{fig:taps10CPUvsGPU} shows execution time for a varying length complex signal convolved with a $10$ tap complex filter.
Contrary to the text book number of flops comparison, convolution in the frequency domain is never faster than time domain in GPUs for a $10$ tap filter, even with an extremely long signal.
Time domain convolution using shared memory in ConvGPUshared is fastest for a signal of significant length.
ConvGPUshared performs very well because only $10$ threads per thread block need to access global memory for the filter coefficients while all other threads only need to access global memory for the signal.
\begin{figure}
	\caption{Lower bounded plot showing trade offs with convolution in GPUs.}
	\centering\includegraphics[width=5in]{figures/gpu_intro/taps10CPUvsGPU.eps}
	\label{fig:taps10CPUvsGPU}
\end{figure}

Long story short, most of the time a GPU DSP engineer is given a set length of signal and filter for convolution.
As Figures \ref{fig:CPUvsGPU_spikes} through \ref{fig:CPUvsGPU_GPUtoGPU} have shown, unless every implementation is explored, there is no way of saying which implementation is fastest.

\section{Thread Optimization}
When first introduced to GPUs, a programmer might to tempted to launch as many threads per block possible.
But, notice in Listing \ref{code:convFun} lines $190$, $203$, $219$ and $225$ launch a minimum of $96$ threads per block and maximum of $192$ threads per block.
Running the GPU with less threads per block with lower occupancy leaves each thread with more resources and memory bandwidth.
Running the GPU with more threads per block with higher occupancy utilizes the full parallel might of the GPU.
If $1024$ threads per block were always launched, lighter computation GPU kernels will be starving for memory while other thread blocks eat up the memory bandwidth.
If $32$ threads per block were always launched, heavier computation GPU kernels will be swimming in resources.

Improving memory accesses should always be the first optimization when a GPU kernel needs to be faster.
The next step is to find the optimal number of threads per block to launch.
Knowing the perfect number of threads per block to launch is challenging to calculate.
Luckily, there is a finite number of possible threads per block, $1$ to $1024$.
A simple test program could time a GPU kernel while sweeping the number of threads per block from $1$ to $1024$.
The number of threads per block with the fastest computation time is the optimal number of threads per block for that specific GPU kernel.

Most of the time the optimal number of threads per block is a multiple of $32$. 
At the lowest level of architecture, GPUs do computations in \textit{warps}.
Warps are groups of $32$ threads that do every computation together in lock step.
If the number of threads per block is a non multiple of $32$, some threads in a warp will be idle and the GPU will have unused resources.

Figure \ref{fig:ConvGPU_shared_12672_186taps} shows the execution time of ConvGPUshared while varying threads per block.
Although the minimum execution time is $0.1078$ms at the optimal $96$ threads per block, incorrect output will result if ConvGPUshared is launched with less than $186$ threads per block.
Launching $96$ threads per block only transfer $96$ filter coefficients to shared memory from global memory.
Luckily, launching $192$ threads per block is near optimal with an execution time of $0.1101$ms.
By simply adjusting the number of threads per block, ConvGPUshared can have a $2\times$ speed up.

Adjusting the number of threads per block doesn't always drastically speed up GPU kernels.
Figure \ref{fig:ConvGPU_shared_12672_186taps} shows the execution time for ConvGPU with varying threads per block.
Launching $560$ does produce about a $1.12$x speed up, but thread optimization doesn't have as much of an affect of ConvGPU verse ConvGPUshared.
\begin{figure}
	\caption{The GPU convolution thread optimization of a $12672$ length signal with a $186$ tap filter using shared memory. $192$ is the optimal number of threads per block executing in $0.1101$ms. Note that at least $186$ threads per block must be launched to compute correct output.}
	\centering\includegraphics[width=5in]{figures/gpu_intro/ConvGPU_shared_12672_186taps.eps}
	\label{fig:ConvGPU_shared_12672_186taps}
\end{figure}
\begin{figure}
	\caption{ConvGPU thread optimization 128 threads per block 0.006811.}
	\centering\includegraphics[width=5in]{figures/gpu_intro/ConvGPU_global_12672_186taps.eps}
	\label{fig:ConvGPU_global_12672_186taps}
\end{figure}

To answer the question: ``There are $X$ number of ways to implement this algorithm, which one is executes the fastest?'' the answer always is ``It depends. Implement the algorithm $X$ ways and see which is fastest.''

\singlespacing
\clearpage
\begin{lstlisting}[caption={CUDA code to performing complex convolution four different ways: time domain CPU, time domain GPU, time domain GPU using shared memory and frequency domain GPU.},label={code:convFun}]
#include <cufft.h>
using namespace std;
// Length of Filter (186 is a magic number)
const int LH = 186;

void ConvCPU(cufftComplex* y,cufftComplex* x,cufftComplex* h,int Lx,int Lh){
	for(int yIdx = 0; yIdx < Lx+Lh-1; yIdx++){
		cufftComplex temp;
		temp.x = 0;
		temp.y = 0;
		for(int hIdx = 0; hIdx < Lh; hIdx++){
			int xAccessIdx = yIdx-hIdx;
			if(xAccessIdx>=0 && xAccessIdx<Lx){
				// temp += x[xAccessIdx]*h[hIdx];
				// (A+jB)(C+jD) = (AC-BD) + j(AD+BC)
				float A = x[xAccessIdx].x;
				float B = x[xAccessIdx].y;
				float C = h[hIdx].x;
				float D = h[hIdx].y;
				cufftComplex complexMult;
				complexMult.x = A*C-B*D;
				complexMult.y = A*D+B*C;

				temp.x += complexMult.x;
				temp.y += complexMult.y;
			}
		}
		y[yIdx] = temp;
	}
}

__global__ void ConvGPU(cufftComplex* y,cufftComplex* x,cufftComplex* h,int Lx,int Lh){
	int yIdx = blockIdx.x*blockDim.x + threadIdx.x;
	int lastThread = Lx+Lh-1;
	// Don't access elements out of bounds
	if(yIdx >= lastThread)
		return;
	cufftComplex temp;
	temp.x = 0;
	temp.y = 0;
	for(int hIdx = 0; hIdx < Lh; hIdx++){
		int xAccessIdx = yIdx-hIdx;
		if(xAccessIdx>=0 && xAccessIdx<Lx){
			// temp += x[xAccessIdx]*h[hIdx];
			// (A+jB)(C+jD) = (AC-BD) + j(AD+BC)
			float A = x[xAccessIdx].x;
			float B = x[xAccessIdx].y;
			float C = h[hIdx].x;
			float D = h[hIdx].y;
			cufftComplex complexMult;
			complexMult.x = A*C-B*D;
			complexMult.y = A*D+B*C;

			temp.x += complexMult.x;
			temp.y += complexMult.y;
		}
		y[yIdx] = temp;
	}
}

__global__ void ConvGPUshared(cufftComplex* y,cufftComplex* x,cufftComplex* h,int Lx,int Lh){
	int yIdx = blockIdx.x*blockDim.x + threadIdx.x;
	// Be sure to read in full h_shared before checking lastThread
	// First Lh threads in thread block read h from global memory into shared memory
	__shared__ cufftComplex h_shared[LH];
	if(threadIdx.x < LH)
		h_shared[threadIdx.x] = h[threadIdx.x];
	int lastThread = Lx+Lh-1;
	// Don't access elements out of bounds
	if(yIdx >= lastThread)
		return;
	// Thread barrier, ensures threads wait until h has been read into h_shared
	__syncthreads();
	cufftComplex temp;
	temp.x = 0;
	temp.y = 0;
	for(int hIdx = 0; hIdx < Lh; hIdx++){
		int xAccessIdx = yIdx-hIdx;
		if(xAccessIdx>=0 && xAccessIdx<Lx){
			// temp += x[xAccessIdx]*h_shared[hIdx];

			// (A+jB)(C+jD) = (AC-BD) + j(AD+BC)
			float A = x[xAccessIdx].x;
			float B = x[xAccessIdx].y;
			float C = h_shared[hIdx].x;
			float D = h_shared[hIdx].y;
			cufftComplex complexMult;
			complexMult.x = A*C-B*D;
			complexMult.y = A*D+B*C;

			temp.x += complexMult.x;
			temp.y += complexMult.y;
		}
		y[yIdx] = temp;
	}
}

__global__ void PointToPointMultiply(cufftComplex* vec0, cufftComplex* vec1, int lastThread){
	int i = blockIdx.x*blockDim.x + threadIdx.x;
	// Don't access elements out of bounds
	if(i >= lastThread)
		return;
	// vec0[i] = vec0[i]*vec1[i]];
	// (A+jB)(C+jD) = (AC-BD) + j(AD+BC)
	float A = vec0[i].x;
	float B = vec0[i].y;
	float C = vec1[i].x;
	float D = vec1[i].y;
	cufftComplex complexMult;
	complexMult.x = A*C-B*D;
	complexMult.y = A*D+B*C;
	vec0[i] = complexMult;
}

__global__ void ScalarMultiply(cufftComplex* vec0, float scalar, int lastThread){
	int i = blockIdx.x*blockDim.x + threadIdx.x;
	// Don't access elements out of bounds
	if(i >= lastThread)
		return;
	cufftComplex scalarMult;
	scalarMult.x = vec0[i].x*scalar;
	scalarMult.y = vec0[i].y*scalar;
	vec0[i] = scalarMult;
}

int main(){
	int mySignalLength = pow(2,15);
	int myFilterLength = LH;
	int myConvLength   = mySignalLength + myFilterLength - 1;
	int Nfft        = pow(2, ceil(log(myConvLength)/log(2)));
	int numTreadsPerBlock;
	int numBlocks;

	cufftHandle plan;
	int n[1] = {Nfft};
	cufftPlanMany(&plan,1,n,NULL,1,1,NULL,1,1,CUFFT_C2C,1);

	// Allocate memory on host
	cufftComplex *mySignal1;
	cufftComplex *myFilter1;
	cufftComplex *myConv1;
	cufftComplex *myConv2;
	cufftComplex *myConv3;
	cufftComplex *myConv4;
	mySignal1      = (cufftComplex*) malloc(mySignalLength*sizeof(cufftComplex));
	myFilter1      = (cufftComplex*) malloc(myFilterLength*sizeof(cufftComplex));
	myConv1        = (cufftComplex*) malloc(myConvLength  *sizeof(cufftComplex));
	myConv2        = (cufftComplex*) malloc(myConvLength  *sizeof(cufftComplex));
	myConv3        = (cufftComplex*) malloc(myConvLength  *sizeof(cufftComplex));
	myConv4        = (cufftComplex*) malloc(Nfft       *sizeof(cufftComplex));
	for(int i = 0; i < mySignalLength; i++){
		mySignal1[i].x = rand()%100-50;
		mySignal1[i].y = rand()%100-50;
	}
	for(int i = 0; i < myFilterLength; i++){
		myFilter1[i].x = rand()%100-50;
		myFilter1[i].y = rand()%100-50;
	}

	// Allocate memory on device
	cufftComplex *dev_mySignal2;
	cufftComplex *dev_myFilter2;
	cufftComplex *dev_myConv2;
	cufftComplex *dev_mySignal3;
	cufftComplex *dev_myFilter3;
	cufftComplex *dev_myConv3;
	cufftComplex *dev_mySignal4;
	cufftComplex *dev_myFilter4;
	cufftComplex *dev_myConv4;
	cudaMalloc(&dev_mySignal2, mySignalLength*sizeof(cufftComplex));
	cudaMalloc(&dev_myFilter2, myFilterLength*sizeof(cufftComplex));
	cudaMalloc(&dev_myConv2,   myConvLength  *sizeof(cufftComplex));
	cudaMalloc(&dev_mySignal3, mySignalLength*sizeof(cufftComplex));
	cudaMalloc(&dev_myFilter3, myFilterLength*sizeof(cufftComplex));
	cudaMalloc(&dev_myConv3,   myConvLength  *sizeof(cufftComplex));
	cudaMalloc(&dev_mySignal4, Nfft          *sizeof(cufftComplex));
	cudaMalloc(&dev_myFilter4, Nfft          *sizeof(cufftComplex));
	cudaMalloc(&dev_myConv4,   Nfft          *sizeof(cufftComplex));

	/*---------------------------------------------------------------------------------------
                                CPU Time Domain Direct Convolution
	---------------------------------------------------------------------------------------*/
	ConvCPU(myConv1,mySignal1,myFilter1,mySignalLength,myFilterLength);

	/*---------------------------------------------------------------------------------------
                                GPU Time DomainDirect Convolution
	---------------------------------------------------------------------------------------*/
	cudaMemcpy(dev_mySignal2, mySignal1, sizeof(cufftComplex)*mySignalLength, cudaMemcpyHostToDevice);
	cudaMemcpy(dev_myFilter2, myFilter1, sizeof(cufftComplex)*myFilterLength, cudaMemcpyHostToDevice);
	numTreadsPerBlock = 128;
	numBlocks = myConvLength/numTreadsPerBlock;
	if(myConvLength % numTreadsPerBlock > 0)
		numBlocks++;
	ConvGPU<<<numBlocks, numTreadsPerBlock>>>(dev_myConv2, dev_mySignal2, dev_myFilter2, mySignalLength, myFilterLength);
	cudaMemcpy(myConv2, dev_myConv2, myConvLength*sizeof(cufftComplex), cudaMemcpyDeviceToHost);


	/*---------------------------------------------------------------------------------------
                                GPU Time Domain Convolution Using Shared Memory
	---------------------------------------------------------------------------------------*/
	cudaMemcpy(dev_mySignal3, mySignal1, sizeof(cufftComplex)*mySignalLength, cudaMemcpyHostToDevice);
	cudaMemcpy(dev_myFilter3, myFilter1, sizeof(cufftComplex)*myFilterLength, cudaMemcpyHostToDevice);
	numTreadsPerBlock = 192;
	numBlocks = myConvLength/numTreadsPerBlock;
	if(myConvLength % numTreadsPerBlock > 0)
		numBlocks++;
	ConvGPUshared<<<numBlocks, numTreadsPerBlock>>>(dev_myConv3, dev_mySignal3, dev_myFilter3, mySignalLength, myFilterLength);
	cudaMemcpy(myConv3, dev_myConv3, myConvLength*sizeof(cufftComplex), cudaMemcpyDeviceToHost);

	/*---------------------------------------------------------------------------------------
                                GPU Frequency Domain Convolution using cuFFT
	---------------------------------------------------------------------------------------*/
	cudaMemset(dev_mySignal4, 0, Nfft*sizeof(cufftComplex));
	cudaMemset(dev_myFilter4, 0, Nfft*sizeof(cufftComplex));
	cudaMemcpy(dev_mySignal4, mySignal1, sizeof(cufftComplex)*mySignalLength, cudaMemcpyHostToDevice);
	cudaMemcpy(dev_myFilter4, myFilter1, sizeof(cufftComplex)*myFilterLength, cudaMemcpyHostToDevice);
	cufftExecC2C(plan, dev_mySignal4, dev_mySignal4, CUFFT_FORWARD);
	cufftExecC2C(plan, dev_myFilter4, dev_myFilter4, CUFFT_FORWARD);
	numTreadsPerBlock = 96;
	numBlocks = Nfft/numTreadsPerBlock;
	if(Nfft % numTreadsPerBlock > 0)
		numBlocks++;
	PointToPointMultiply<<<numBlocks, numTreadsPerBlock>>>(dev_mySignal4, dev_myFilter4, Nfft);
	cufftExecC2C(plan, dev_mySignal4, dev_mySignal4, CUFFT_INVERSE);
	numTreadsPerBlock = 128;
	numBlocks = Nfft/numTreadsPerBlock;
	if(Nfft % numTreadsPerBlock > 0)
		numBlocks++;
	float scalar = 1.0/((float)Nfft);
	ScalarMultiply<<<numBlocks, numTreadsPerBlock>>>(dev_mySignal4, scalar, Nfft);
	cudaMemcpy(myConv4, dev_mySignal4, myConvLength*sizeof(cufftComplex), cudaMemcpyDeviceToHost);
	cufftDestroy(plan);

	// Free vectors on CPU
	free(mySignal1);
	free(myFilter1);
	free(myConv1);
	free(myConv2);
	free(myConv3);
	free(myConv4);

	// Free vectors on GPU
	cudaFree(dev_mySignal2);
	cudaFree(dev_myFilter2);
	cudaFree(dev_myConv2);
	cudaFree(dev_mySignal2);
	cudaFree(dev_myFilter2);
	cudaFree(dev_myConv2);
	cudaFree(dev_mySignal3);
	cudaFree(dev_myFilter3);
	cudaFree(dev_myConv3);
	cudaFree(dev_mySignal4);
	cudaFree(dev_myFilter4);
	cudaFree(dev_myConv4);

	return 0;
}
\end{lstlisting}
\doublespacing