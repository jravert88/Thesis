\chapter{Problem Statement}
This is the Problem Statement

Some algorithms map very well to CPUs because they are computationally light,
but what happens why your CPU cannot acheive the desired throughput or data rate?

In the past, the answer was FPGAs. But now, with Graphics Processing Units getting bigger faster stronger, there has been a recent pull towards GPUs because of

the ease of implementation vs HDL programming

the ease of setup


A Graphics Processing Unit (GPU) is a computational unit with a specialized, highly-parallel architecture well-suited to processing large blocks of data.
The performance advantage derives from the GPUs ability to perform a large number of parallel computations on the large data block.
Historically, the large block of data was graphics data and the computations were limited to those required for graphics operations.
Since about 2006, there has been increasing interest in using genral purpose GPUs for more general processing tasks such as machine learning, oil exploration, scientific image processing, linear algebra, statistics, and 3D reconstruction \cite{wikipedia-gpu:2015}.
This project leverages this trend and applies GPU processing to the estimation, computation, and filtering operations required for data-aided equalization.

Typically, the GPU architecture comprises a large memory block accessible by several (up to a few thousand) processing units simultaneously.
For example, memory and processing units (called ``CUDA cores'') for the two Nvidia GPUs used in this project are summarized in Table~\ref{tab:gpu-resources}.
Consequently, algorithms that are highly parallel are best suited for the GPU.
In constrast, sequential algorithms (e.g., a phase lock loop!) are not well suited for the GPU.

Programming the Nvidia GPUs is written in the C++ computer language with API extensions known as CUDA (Compute Unified Device Architecture).
The CUDA API is ``a software layer that gives direct access to the GPU's virtual instruction set and parallel computational elements'' \cite{wikipedia-cuda:2015}.
CUDA allows the C++ program, running on the host CPU, to launch special functions---known as kernels--- on the GPU while allowing some of the functionality to remain on the host CPU.

The choice to use GPUs, rather than FPGAs, for this project include the following:
\begin{enumerate}
	\item The desire to test the performance of four equalizers operating in parallel on the
		received data is a good match to GPU structure.
	\item Programming in C++ has advantages over VHDL designs in an FPGA.
		Development time is much shorter and debugging is quite a bit easier.
		Small modifications are much easier with C++ and GPUs than with VHDL in and FPGA.
	\item  Because the GPU supports floating point operations, the complications of tracking
		``digit growth'' in fixed-point operations (on an FPGA) are eliminated.
\end{enumerate}
GPUs are unlikely to be used for telemetry demodulators in the foreseeable future.
However, because the point of the project is to assess the performance of competing algorithms,
this fact is less important.
Once identified, the ``best'' equalizer algorithm can be designed in VHDL and incorporated
into the FPGA-based demodulators commonly found on the telemetry market.

The unique features of the GPU architecture require the designer to rethink how the
signal processing is organized.
DRAM memory limitations on the FPGA limit the number of samples per transfer $39{,}321{,}600$
complex-valued samples ($314{,}572{,}800$ Bytes).
This data, corresponding to $3{,}103$ packets,
is loaded into the GPU memory (cf. Table~\ref{tab:gpu-resources}).
Here starting indexes of the $3{,}103$ occurrences of the preamble are found.
Subsequent signal processing for each packet is performed \textit{in parallel}.
In the end, $3{,}103 \times 6{,}144 = 19{,}064{,}832$ data bits \textit{per equalizer}
are produced at the end of the processing applied to each block.
A conceptual block diagram of this organization is illustrated in Figure~\ref{fig:signal-flow-gpu}.
The preamble detector (or frame synchronizer), frequency offset estimator, channel estimator, and
noise variance estimator are described in Section~\ref{sec:estimators}.
The equalizers are described in Section~\ref{sec:eq}.


\begin{table}
\caption{The computational resources available with three Nvidia GPUs used in this project (1x Tesla k40 2x Tesla K20).}
\label{tab:gpu-resources}
\begin{center}
\begin{tabular}{lll}
	\toprule
	Feature & Tesla k40 & Tesla K20 \\ \midrule
	Memory size (GDDR5) & 12 GB & 5 GB \\
	CUDA cores & 2880 & 2496 \\ \bottomrule
\end{tabular}
\end{center}
\end{table}
