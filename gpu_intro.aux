\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{wikipedia-gpu:2015}
\citation{CUDA_toolkit_doc}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Signal Processing with GPUs}{5}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:gpu}{{3}{5}{Signal Processing with GPUs}{chapter.3}{}}
\@writefile{brf}{\backcite{wikipedia-gpu:2015}{{5}{3}{chapter.3}}}
\@writefile{brf}{\backcite{CUDA_toolkit_doc}{{5}{3}{chapter.3}}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Simple GPU code example}{5}{section.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A block diagram of how a CPU sequentially performs vector addition.}}{6}{figure.3.1}}
\newlabel{fig:CPUaddBlockDiagram}{{3.1}{6}{Simple GPU code example}{figure.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A block diagram of how a GPU performs vector addition in parallel.}}{6}{figure.3.2}}
\newlabel{fig:GPUaddBlockDiagram}{{3.2}{6}{Simple GPU code example}{figure.3.2}{}}
\newlabel{code:GPUvsCPU}{{3.1}{7}{Comparison of CPU verse GPU code}{lstlisting.3.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.1}Comparison of CPU verse GPU code.}{7}{lstlisting.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}GPU kernel using threads and thread blocks}{8}{section.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Block $0$ $32$ threads launched in $4$ thread blocks with $8$ threads per block.}}{9}{figure.3.3}}
\newlabel{fig:threadsBlocks32}{{3.3}{9}{GPU kernel using threads and thread blocks}{figure.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces $36$ threads launched in $5$ thread blocks with $8$ threads per block with $4$ idle threads.}}{9}{figure.3.4}}
\newlabel{fig:threadsBlocks36}{{3.4}{9}{GPU kernel using threads and thread blocks}{figure.3.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}GPU memory}{9}{section.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces A block diagram where local, shared, and global memory is located. Each thread has private local memory. Each thread block has private shared memory. The GPU has global memory that all threads can access.}}{10}{figure.3.5}}
\newlabel{fig:fullGPUmemBlockDiagram}{{3.5}{10}{GPU memory}{figure.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The computational resources available with three NVIDIA GPUs used in this thesis (1x Tesla K40c 2x Tesla K20c).}}{10}{table.3.1}}
\newlabel{tab:gpu-resources_jeffs}{{3.1}{10}{GPU memory}{table.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces NVIDIA Tesla K40c and K20c.}}{11}{figure.3.6}}
\newlabel{fig:GPUpicture}{{3.6}{11}{GPU memory}{figure.3.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Example of an NVIDIA GPU card. The SRAM is shown to be boxed in yellow. The GPU chip is shown to be boxed in red.}}{11}{figure.3.7}}
\newlabel{fig:GPUarch}{{3.7}{11}{GPU memory}{figure.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Cuda Libraries}{12}{section.3.4}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Cuda Convolution}{12}{section.3.5}}
\newlabel{eq:simple_conv}{{3.2}{12}{Cuda Convolution}{equation.3.5.2}{}}
\citation{Cooley-Tukey_old}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Defining start and stop lines for timing comparison in Listing \ref  {code:convFun}.}}{14}{table.3.2}}
\newlabel{tab:CPUvsGPUtimingTable}{{3.2}{14}{Cuda Convolution}{table.3.2}{}}
\@writefile{brf}{\backcite{Cooley-Tukey_old}{{14}{3.5}{figure.3.10}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Comparison of complex convolution on CPU to GPU with varying signal lengths without lower bounding.}}{15}{figure.3.8}}
\newlabel{fig:CPUvsGPU_spikes}{{3.8}{15}{Cuda Convolution}{figure.3.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Convolution computation times with signal length $2^{15} = 32768$ and filter length $186$ on a Tesla K40c GPU.}}{15}{table.3.3}}
\newlabel{tab:CPUvsGPUtable_2_15}{{3.3}{15}{Cuda Convolution}{table.3.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Convolution computation times with signal length $12672$ and filter length $186$ on a Tesla K40c GPU.}}{15}{table.3.4}}
\newlabel{tab:CPUvsGPUtable_12672}{{3.4}{15}{Cuda Convolution}{table.3.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Comparison of complex convolution on CPU to GPU with varying signal lengths with lower bounding.}}{16}{figure.3.9}}
\newlabel{fig:CPUvsGPU}{{3.9}{16}{Cuda Convolution}{figure.3.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Plot showing when CPU convolution is faster than GPU convolution.}}{17}{figure.3.10}}
\newlabel{fig:CPUvsGPU_CPUtoGPU}{{3.10}{17}{Cuda Convolution}{figure.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Thread Optimization}{17}{section.3.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Plot showing trade offs with convolution in GPUs.}}{18}{figure.3.11}}
\newlabel{fig:CPUvsGPU_GPUtoGPU}{{3.11}{18}{Cuda Convolution}{figure.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Lower bounded plot showing trade offs with convolution in GPUs.}}{19}{figure.3.12}}
\newlabel{fig:taps10CPUvsGPU}{{3.12}{19}{Cuda Convolution}{figure.3.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces The GPU convolution thread optimization of a $12672$ length signal with a $186$ tap filter using shared memory. $192$ is the optimal number of threads per block executing in $0.1101$ms. Note that at least $186$ threads per block must be launched to compute correct output.}}{20}{figure.3.13}}
\newlabel{fig:ConvGPU_shared_12672_186taps}{{3.13}{20}{Thread Optimization}{figure.3.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}CPU GPU Pipelining}{20}{section.3.7}}
\newlabel{code:noPipe}{{3.2}{20}{Example code Simple example of the CPU acquiring data from myADC, copying from host to device, processing data on the device then copying from device to host. No processing occurs on device while CPU is acquiring data}{lstlisting.3.2}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.2}Example code Simple example of the CPU acquiring data from myADC, copying from host to device, processing data on the device then copying from device to host. No processing occurs on device while CPU is acquiring data.}{20}{lstlisting.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces ConvGPU thread optimization 128 threads per block 0.006811.}}{21}{figure.3.14}}
\newlabel{fig:ConvGPU_global_12672_186taps}{{3.14}{21}{Thread Optimization}{figure.3.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The typical approach of CPU and GPU operations. This block diagram shows a Profile of Listing \ref  {code:noPipe}.}}{21}{figure.3.15}}
\newlabel{fig:concurrentCPU_nonBlocking}{{3.15}{21}{CPU GPU Pipelining}{figure.3.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces GPU and CPU operations can be pipelined. This block diagram shows a Profile of Listing \ref  {code:pipe}.}}{22}{figure.3.16}}
\newlabel{fig:concurrentCPU_blocking}{{3.16}{22}{CPU GPU Pipelining}{figure.3.16}{}}
\newlabel{code:pipe}{{3.3}{22}{Example code Simple of the CPU acquiring data from myADC, copying from host to device, processing data on the device then copying from device to host. No processing occurs on device while CPU is acquiring data}{lstlisting.3.3}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.3}Example code Simple of the CPU acquiring data from myADC, copying from host to device, processing data on the device then copying from device to host. No processing occurs on device while CPU is acquiring data.}{22}{lstlisting.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces A block diagram of pipelining a CPU with three GPUs.}}{23}{figure.3.17}}
\newlabel{fig:concurrentCPU_nonBlocking_multiGPU}{{3.17}{23}{CPU GPU Pipelining}{figure.3.17}{}}
\newlabel{code:convFun}{{3.4}{24}{CUDA code to performing complex convolution four different ways: time domain CPU, time domain GPU, time domain GPU using shared memory and frequency domain GPU}{lstlisting.3.4}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {3.4}CUDA code to performing complex convolution four different ways: time domain CPU, time domain GPU, time domain GPU using shared memory and frequency domain GPU.}{24}{lstlisting.3.4}}
\@setckpt{gpu_intro}{
\setcounter{page}{28}
\setcounter{equation}{2}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{7}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{17}
\setcounter{table}{4}
\setcounter{parentequation}{0}
\setcounter{r@tfl@t}{0}
\setcounter{cp@cnt}{0}
\setcounter{cp@tempcnt}{0}
\setcounter{lstnumber}{258}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{15}
\setcounter{lstlisting}{4}
\setcounter{section@level}{1}
}
